{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W9QpA0RFXaeG"
   },
   "outputs": [],
   "source": [
    "# Set up imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import model_flower\n",
    "import model_train\n",
    "import model_test\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Set up variables\n",
    "tr_batchsize = 16 # The size of the training batches\n",
    "val_test_batchsize = 16 # The size of the validation / testing batches\n",
    "epochs = 500 # The number of epochs to do\n",
    "validate_steps = 750 # The number of steps to complete before validation\n",
    "learning_rate = 0.00005 # The learning rate to start at\n",
    "load_model = False # If a model should be requested to be loaded, or not\n",
    "save_model = True # If the model should be saved after testing, or not\n",
    "\n",
    "# The actual model variables\n",
    "model = None\n",
    "criterion = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "# Model file values. If \"None\", then they haven't loaded successfully\n",
    "md = None\n",
    "lr = None\n",
    "sch = None\n",
    "cri = None\n",
    "opt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs.\n",
      "Now the deivce is set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# By default, set to use the CPU\n",
    "deviceFlag = torch.device('cpu')\n",
    "\n",
    "# If a GPU is available, use it\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
    "    deviceFlag = torch.device('cuda:0') # Default to cuda 0, but can be changed.\n",
    "\n",
    "print(f'Now the deivce is set to {deviceFlag}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading and Transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    # Randomly rotate it 90 degrees\n",
    "    transforms.RandomRotation(90),\n",
    "    # Randomly sharpen the image\n",
    "    transforms.RandomAdjustSharpness(1.5, 0.5),\n",
    "    # Randomly crop an area of the flower of size 224x224\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    # Flip it horizontally, or don't\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Flip it vertically, or don't\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # Convert the image to a Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the Tensor values so that they're easier for the\n",
    "    # model to train from\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets of the Flower102 images\n",
    "train_dataset = datasets.Flowers102(root = './dataset', split = 'train', transform = training_transforms, download = True)\n",
    "valid_dataset = datasets.Flowers102(root = './dataset', split = 'val', transform = validation_transforms, download = True)\n",
    "test_dataset = datasets.Flowers102(root = './dataset', split = 'test', transform = testing_transforms, download = True)\n",
    "\n",
    "\n",
    "# Create the loaders for the datasets, to be used to train, validate and test the model\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = tr_batchsize,\n",
    "                                           shuffle = True)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                           batch_size = val_test_batchsize)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = val_test_batchsize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Model created. Moving the Model to cuda...\n",
      "Moved the Model to cuda.\n"
     ]
    }
   ],
   "source": [
    "loaded_file = False\n",
    "\n",
    "file_name = \"ERROR\"\n",
    "# If going to load a model\n",
    "if load_model:\n",
    "    # First request the file name of a model\n",
    "    file_name = \"models/\" + input(\"Model to load: \")\n",
    "    # And try to load it\n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            file_data = torch.load(file_name)\n",
    "            md = file_data[\"model\"]\n",
    "            lr = file_data[\"learning_rate\"]\n",
    "            sch = file_data[\"scheduler\"]\n",
    "            cri = file_data[\"criterion\"]\n",
    "            opt = file_data[\"optimizer\"]\n",
    "            loaded_file = True\n",
    "            print(\"Model loaded.\")\n",
    "        except:\n",
    "            # If it fails, load nothing from the file\n",
    "            md = None\n",
    "            lr = learning_rate\n",
    "            sch = None\n",
    "            cri = None\n",
    "            opt = None\n",
    "            print(\"Model failed to load. Using default untrained Model.\")\n",
    "\n",
    "print(\"Creating Model...\")\n",
    "model = model_flower.FlowerModel()\n",
    "print(\"Model created. Moving the Model to \" + deviceFlag.type + \"...\")\n",
    "model.to(deviceFlag)\n",
    "print(\"Moved the Model to \" + deviceFlag.type + \".\")\n",
    "\n",
    "if loaded_file:\n",
    "    print(\"\\nUsing model file from \" + file_name)\n",
    "    model.load_state_dict(md)\n",
    "    learning_rate = lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Loss Function and Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if not cri is None:\n",
    "    criterion.load_state_dict(cri)\n",
    "\n",
    "# optimizer 1\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "if not opt is None:\n",
    "    optimizer.load_state_dict(opt)\n",
    "\n",
    "# optimizer 2\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 500, 0.99)\n",
    "if not sch is None:\n",
    "    scheduler.load_state_dict(sch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] Epoch 1 completed on Batch 64 in 18.5239 seconds (18.5239 in total) \n",
      "\n",
      "[2/500] Epoch 2 completed on Batch 128 in 16.2534 seconds (34.7773 in total) \n",
      "\n",
      "[3/500] Epoch 3 completed on Batch 192 in 15.9812 seconds (50.7585 in total) \n",
      "\n",
      "[4/500] Epoch 4 completed on Batch 256 in 15.6626 seconds (66.4211 in total) \n",
      "\n",
      "[5/500] Epoch 5 completed on Batch 320 in 15.8073 seconds (82.2285 in total) \n",
      "\n",
      "[6/500] Epoch 6 completed on Batch 384 in 15.5081 seconds (97.7366 in total) \n",
      "\n",
      "[7/500] Epoch 7 completed on Batch 448 in 15.8367 seconds (113.5733 in total) \n",
      "\n",
      "[8/500] Epoch 8 completed on Batch 512 in 15.8523 seconds (129.4256 in total) \n",
      "\n",
      "[9/500] Epoch 9 completed on Batch 576 in 16.0404 seconds (145.4660 in total) \n",
      "\n",
      "[10/500] Epoch 10 completed on Batch 640 in 15.6658 seconds (161.1318 in total) \n",
      "\n",
      "[11/500] Epoch 11 completed on Batch 704 in 15.7133 seconds (176.8451 in total) \n",
      "\n",
      "[12/500] Batch: 750... Training Loss since last stepped validation: 3.4915... Validation Loss: 3.0881... Validation Accuracy: 0.2259\n",
      "[12/500] Epoch 12 completed on Batch 768 in 24.2762 seconds (201.1214 in total) \n",
      "\n",
      "[13/500] Epoch 13 completed on Batch 832 in 15.8769 seconds (216.9983 in total) \n",
      "\n",
      "[14/500] Epoch 14 completed on Batch 896 in 15.7491 seconds (232.7474 in total) \n",
      "\n",
      "[15/500] Epoch 15 completed on Batch 960 in 15.9920 seconds (248.7393 in total) \n",
      "\n",
      "[16/500] Epoch 16 completed on Batch 1024 in 15.7495 seconds (264.4888 in total) \n",
      "\n",
      "[17/500] Epoch 17 completed on Batch 1088 in 15.8433 seconds (280.3322 in total) \n",
      "\n",
      "[18/500] Epoch 18 completed on Batch 1152 in 16.0418 seconds (296.3739 in total) \n",
      "\n",
      "[19/500] Epoch 19 completed on Batch 1216 in 15.9439 seconds (312.3179 in total) \n",
      "\n",
      "[20/500] Epoch 20 completed on Batch 1280 in 15.7999 seconds (328.1177 in total) \n",
      "\n",
      "[21/500] Epoch 21 completed on Batch 1344 in 15.7850 seconds (343.9027 in total) \n",
      "\n",
      "[22/500] Epoch 22 completed on Batch 1408 in 15.7313 seconds (359.6340 in total) \n",
      "\n",
      "[23/500] Epoch 23 completed on Batch 1472 in 15.8742 seconds (375.5082 in total) \n",
      "\n",
      "[24/500] Batch: 1500... Training Loss since last stepped validation: 2.9473... Validation Loss: 3.0821... Validation Accuracy: 0.2191\n",
      "[24/500] Epoch 24 completed on Batch 1536 in 24.5537 seconds (400.0619 in total) \n",
      "\n",
      "[25/500] Epoch 25 completed on Batch 1600 in 15.7130 seconds (415.7749 in total) \n",
      "\n",
      "[26/500] Epoch 26 completed on Batch 1664 in 15.9576 seconds (431.7325 in total) \n",
      "\n",
      "[27/500] Epoch 27 completed on Batch 1728 in 15.8937 seconds (447.6262 in total) \n",
      "\n",
      "[28/500] Epoch 28 completed on Batch 1792 in 15.8434 seconds (463.4696 in total) \n",
      "\n",
      "[29/500] Epoch 29 completed on Batch 1856 in 15.8350 seconds (479.3046 in total) \n",
      "\n",
      "[30/500] Epoch 30 completed on Batch 1920 in 15.8213 seconds (495.1259 in total) \n",
      "\n",
      "[31/500] Epoch 31 completed on Batch 1984 in 15.7831 seconds (510.9091 in total) \n",
      "\n",
      "[32/500] Epoch 32 completed on Batch 2048 in 15.8825 seconds (526.7915 in total) \n",
      "\n",
      "[33/500] Epoch 33 completed on Batch 2112 in 15.8223 seconds (542.6138 in total) \n",
      "\n",
      "[34/500] Epoch 34 completed on Batch 2176 in 15.7626 seconds (558.3765 in total) \n",
      "\n",
      "[35/500] Epoch 35 completed on Batch 2240 in 16.0224 seconds (574.3989 in total) \n",
      "\n",
      "[36/500] Batch: 2250... Training Loss since last stepped validation: 2.9475... Validation Loss: 3.1072... Validation Accuracy: 0.2139\n",
      "[36/500] Epoch 36 completed on Batch 2304 in 24.5849 seconds (598.9838 in total) \n",
      "\n",
      "[37/500] Epoch 37 completed on Batch 2368 in 16.0327 seconds (615.0165 in total) \n",
      "\n",
      "[38/500] Epoch 38 completed on Batch 2432 in 15.8285 seconds (630.8450 in total) \n",
      "\n",
      "[39/500] Epoch 39 completed on Batch 2496 in 16.1361 seconds (646.9811 in total) \n",
      "\n",
      "[40/500] Epoch 40 completed on Batch 2560 in 15.6923 seconds (662.6734 in total) \n",
      "\n",
      "[41/500] Epoch 41 completed on Batch 2624 in 15.7599 seconds (678.4333 in total) \n",
      "\n",
      "[42/500] Epoch 42 completed on Batch 2688 in 15.8639 seconds (694.2972 in total) \n",
      "\n",
      "[43/500] Epoch 43 completed on Batch 2752 in 15.7816 seconds (710.0788 in total) \n",
      "\n",
      "[44/500] Epoch 44 completed on Batch 2816 in 15.9806 seconds (726.0594 in total) \n",
      "\n",
      "[45/500] Epoch 45 completed on Batch 2880 in 15.9079 seconds (741.9673 in total) \n",
      "\n",
      "[46/500] Epoch 46 completed on Batch 2944 in 15.8878 seconds (757.8551 in total) \n",
      "\n",
      "[47/500] Batch: 3000... Training Loss since last stepped validation: 2.9499... Validation Loss: 3.0955... Validation Accuracy: 0.2217\n",
      "[47/500] Epoch 47 completed on Batch 3008 in 24.6295 seconds (782.4845 in total) \n",
      "\n",
      "[48/500] Epoch 48 completed on Batch 3072 in 15.9209 seconds (798.4055 in total) \n",
      "\n",
      "[49/500] Epoch 49 completed on Batch 3136 in 15.8518 seconds (814.2573 in total) \n",
      "\n",
      "[50/500] Epoch 50 completed on Batch 3200 in 15.8528 seconds (830.1100 in total) \n",
      "\n",
      "[51/500] Epoch 51 completed on Batch 3264 in 15.8919 seconds (846.0019 in total) \n",
      "\n",
      "[52/500] Epoch 52 completed on Batch 3328 in 15.6445 seconds (861.6465 in total) \n",
      "\n",
      "[53/500] Epoch 53 completed on Batch 3392 in 16.0731 seconds (877.7196 in total) \n",
      "\n",
      "[54/500] Epoch 54 completed on Batch 3456 in 16.0316 seconds (893.7512 in total) \n",
      "\n",
      "[55/500] Epoch 55 completed on Batch 3520 in 16.3008 seconds (910.0520 in total) \n",
      "\n",
      "[56/500] Epoch 56 completed on Batch 3584 in 15.9278 seconds (925.9798 in total) \n",
      "\n",
      "[57/500] Epoch 57 completed on Batch 3648 in 15.7318 seconds (941.7116 in total) \n",
      "\n",
      "[58/500] Epoch 58 completed on Batch 3712 in 15.9251 seconds (957.6366 in total) \n",
      "\n",
      "[59/500] Batch: 3750... Training Loss since last stepped validation: 2.9481... Validation Loss: 3.0816... Validation Accuracy: 0.2142\n",
      "[59/500] Epoch 59 completed on Batch 3776 in 24.5350 seconds (982.1717 in total) \n",
      "\n",
      "[60/500] Epoch 60 completed on Batch 3840 in 15.8810 seconds (998.0526 in total) \n",
      "\n",
      "[61/500] Epoch 61 completed on Batch 3904 in 15.8478 seconds (1013.9005 in total) \n",
      "\n",
      "[62/500] Epoch 62 completed on Batch 3968 in 15.6753 seconds (1029.5758 in total) \n",
      "\n",
      "[63/500] Epoch 63 completed on Batch 4032 in 15.6942 seconds (1045.2700 in total) \n",
      "\n",
      "[64/500] Epoch 64 completed on Batch 4096 in 15.9714 seconds (1061.2414 in total) \n",
      "\n",
      "[65/500] Epoch 65 completed on Batch 4160 in 15.7970 seconds (1077.0384 in total) \n",
      "\n",
      "[66/500] Epoch 66 completed on Batch 4224 in 15.8065 seconds (1092.8448 in total) \n",
      "\n",
      "[67/500] Epoch 67 completed on Batch 4288 in 15.7899 seconds (1108.6347 in total) \n",
      "\n",
      "[68/500] Epoch 68 completed on Batch 4352 in 15.6721 seconds (1124.3068 in total) \n",
      "\n",
      "[69/500] Epoch 69 completed on Batch 4416 in 15.6866 seconds (1139.9934 in total) \n",
      "\n",
      "[70/500] Epoch 70 completed on Batch 4480 in 15.7687 seconds (1155.7621 in total) \n",
      "\n",
      "[71/500] Batch: 4500... Training Loss since last stepped validation: 2.9469... Validation Loss: 3.0843... Validation Accuracy: 0.2201\n",
      "[71/500] Epoch 71 completed on Batch 4544 in 24.4012 seconds (1180.1633 in total) \n",
      "\n",
      "[72/500] Epoch 72 completed on Batch 4608 in 16.0532 seconds (1196.2164 in total) \n",
      "\n",
      "[73/500] Epoch 73 completed on Batch 4672 in 15.6794 seconds (1211.8958 in total) \n",
      "\n",
      "[74/500] Epoch 74 completed on Batch 4736 in 15.8053 seconds (1227.7011 in total) \n",
      "\n",
      "[75/500] Epoch 75 completed on Batch 4800 in 15.8607 seconds (1243.5618 in total) \n",
      "\n",
      "[76/500] Epoch 76 completed on Batch 4864 in 15.9486 seconds (1259.5104 in total) \n",
      "\n",
      "[77/500] Epoch 77 completed on Batch 4928 in 15.8888 seconds (1275.3992 in total) \n",
      "\n",
      "[78/500] Epoch 78 completed on Batch 4992 in 16.3058 seconds (1291.7050 in total) \n",
      "\n",
      "[79/500] Epoch 79 completed on Batch 5056 in 15.9951 seconds (1307.7001 in total) \n",
      "\n",
      "[80/500] Epoch 80 completed on Batch 5120 in 15.8149 seconds (1323.5150 in total) \n",
      "\n",
      "[81/500] Epoch 81 completed on Batch 5184 in 15.8449 seconds (1339.3599 in total) \n",
      "\n",
      "[82/500] Epoch 82 completed on Batch 5248 in 15.7671 seconds (1355.1270 in total) \n",
      "\n",
      "[83/500] Batch: 5250... Training Loss since last stepped validation: 2.9459... Validation Loss: 3.0783... Validation Accuracy: 0.2181\n",
      "[83/500] Epoch 83 completed on Batch 5312 in 24.4039 seconds (1379.5309 in total) \n",
      "\n",
      "[84/500] Epoch 84 completed on Batch 5376 in 15.9371 seconds (1395.4679 in total) \n",
      "\n",
      "[85/500] Epoch 85 completed on Batch 5440 in 15.8436 seconds (1411.3115 in total) \n",
      "\n",
      "[86/500] Epoch 86 completed on Batch 5504 in 15.9902 seconds (1427.3017 in total) \n",
      "\n",
      "[87/500] Epoch 87 completed on Batch 5568 in 15.9547 seconds (1443.2564 in total) \n",
      "\n",
      "[88/500] Epoch 88 completed on Batch 5632 in 15.6651 seconds (1458.9215 in total) \n",
      "\n",
      "[89/500] Epoch 89 completed on Batch 5696 in 15.7163 seconds (1474.6379 in total) \n",
      "\n",
      "[90/500] Epoch 90 completed on Batch 5760 in 15.8733 seconds (1490.5112 in total) \n",
      "\n",
      "[91/500] Epoch 91 completed on Batch 5824 in 15.9001 seconds (1506.4113 in total) \n",
      "\n",
      "[92/500] Epoch 92 completed on Batch 5888 in 16.0196 seconds (1522.4310 in total) \n",
      "\n",
      "[93/500] Epoch 93 completed on Batch 5952 in 15.9673 seconds (1538.3983 in total) \n",
      "\n",
      "[94/500] Batch: 6000... Training Loss since last stepped validation: 2.9583... Validation Loss: 3.0915... Validation Accuracy: 0.2210\n",
      "[94/500] Epoch 94 completed on Batch 6016 in 24.3049 seconds (1562.7032 in total) \n",
      "\n",
      "[95/500] Epoch 95 completed on Batch 6080 in 15.9280 seconds (1578.6312 in total) \n",
      "\n",
      "[96/500] Epoch 96 completed on Batch 6144 in 15.7835 seconds (1594.4148 in total) \n",
      "\n",
      "[97/500] Epoch 97 completed on Batch 6208 in 15.7436 seconds (1610.1584 in total) \n",
      "\n",
      "[98/500] Epoch 98 completed on Batch 6272 in 15.6991 seconds (1625.8574 in total) \n",
      "\n",
      "[99/500] Epoch 99 completed on Batch 6336 in 15.8875 seconds (1641.7450 in total) \n",
      "\n",
      "[100/500] Epoch 100 completed on Batch 6400 in 15.7142 seconds (1657.4592 in total) \n",
      "\n",
      "[101/500] Epoch 101 completed on Batch 6464 in 16.0648 seconds (1673.5240 in total) \n",
      "\n",
      "[102/500] Epoch 102 completed on Batch 6528 in 15.8302 seconds (1689.3542 in total) \n",
      "\n",
      "[103/500] Epoch 103 completed on Batch 6592 in 15.9609 seconds (1705.3151 in total) \n",
      "\n",
      "[104/500] Epoch 104 completed on Batch 6656 in 16.1981 seconds (1721.5132 in total) \n",
      "\n",
      "[105/500] Epoch 105 completed on Batch 6720 in 16.6812 seconds (1738.1944 in total) \n",
      "\n",
      "[106/500] Batch: 6750... Training Loss since last stepped validation: 2.9490... Validation Loss: 3.0883... Validation Accuracy: 0.2152\n",
      "[106/500] Epoch 106 completed on Batch 6784 in 25.4221 seconds (1763.6165 in total) \n",
      "\n",
      "[107/500] Epoch 107 completed on Batch 6848 in 16.6560 seconds (1780.2724 in total) \n",
      "\n",
      "[108/500] Epoch 108 completed on Batch 6912 in 15.8950 seconds (1796.1675 in total) \n",
      "\n",
      "[109/500] Epoch 109 completed on Batch 6976 in 15.8072 seconds (1811.9788 in total) \n",
      "\n",
      "[110/500] Epoch 110 completed on Batch 7040 in 16.0835 seconds (1828.0623 in total) \n",
      "\n",
      "[111/500] Epoch 111 completed on Batch 7104 in 15.8241 seconds (1843.8864 in total) \n",
      "\n",
      "[112/500] Epoch 112 completed on Batch 7168 in 15.6329 seconds (1859.5193 in total) \n",
      "\n",
      "[113/500] Epoch 113 completed on Batch 7232 in 15.7899 seconds (1875.3092 in total) \n",
      "\n",
      "[114/500] Epoch 114 completed on Batch 7296 in 16.1272 seconds (1891.4363 in total) \n",
      "\n",
      "[115/500] Epoch 115 completed on Batch 7360 in 16.3568 seconds (1907.7931 in total) \n",
      "\n",
      "[116/500] Epoch 116 completed on Batch 7424 in 15.6221 seconds (1923.4152 in total) \n",
      "\n",
      "[117/500] Epoch 117 completed on Batch 7488 in 16.0468 seconds (1939.4620 in total) \n",
      "\n",
      "[118/500] Batch: 7500... Training Loss since last stepped validation: 2.9655... Validation Loss: 3.0908... Validation Accuracy: 0.2132\n",
      "[118/500] Epoch 118 completed on Batch 7552 in 24.5438 seconds (1964.0058 in total) \n",
      "\n",
      "[119/500] Epoch 119 completed on Batch 7616 in 16.2025 seconds (1980.2083 in total) \n",
      "\n",
      "[120/500] Epoch 120 completed on Batch 7680 in 16.0604 seconds (1996.2687 in total) \n",
      "\n",
      "[121/500] 560 / 1020 - 54.902% complete, 7715 Batches completed."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                             \u001B[49m\u001B[43moptim_scheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_flag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeviceFlag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mvalidate_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_stepped\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mvalidate_end\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mZ:\\Files\\University\\Exams\\INT2\\GitHub Project\\model_train.py:79\u001B[0m, in \u001B[0;36mtrain_classifier\u001B[1;34m(model, train_loader, validate_loader, optimizer, criterion, optim_scheduler, device_flag, epochs, validate_steps, validate_stepped, validate_epoch, validate_end)\u001B[0m\n\u001B[0;32m     76\u001B[0m     optim_scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     78\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m---> 79\u001B[0m total_running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m running_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validate_stepped:\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;66;03m# If not on last step, and on print step\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model_train.train_classifier(model, train_loader, validate_loader, optimizer, criterion,\n",
    "                             optim_scheduler=scheduler, device_flag=deviceFlag, epochs=epochs,\n",
    "                             validate_steps=validate_steps, validate_stepped=True, validate_epoch=False,\n",
    "                             validate_end=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_test.test_accuracy(model, test_loader, device_flag=deviceFlag)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    save_data = {\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"model\": model.state_dict(),\n",
    "        \"criterion\": criterion.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(save_data, \"models/\" + str(datetime.datetime.now()).replace(\":\",\"-\")\n",
    "               + f\" b{tr_batchsize}-e{epochs}-lr{learning_rate}\" \"-model.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop Run All here\n",
    "assert False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reload imports in the case that they are changed\n",
    "from importlib import reload\n",
    "\n",
    "# If not loaded into cache yet, import them\n",
    "import model_flower\n",
    "import model_train\n",
    "import model_test\n",
    "\n",
    "reload(model_flower)\n",
    "reload(model_train)\n",
    "reload(model_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_model = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.525878906250001e-69\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T22:54:06.889406200Z",
     "start_time": "2023-05-07T22:54:06.842310700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
