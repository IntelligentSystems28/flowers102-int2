{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W9QpA0RFXaeG",
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:43.105008900Z",
     "start_time": "2023-05-07T18:35:40.616852900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import model_flower\n",
    "import model_train\n",
    "import model_test\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Set up variables\n",
    "tr_batchsize = 16 # The size of the training batches\n",
    "val_test_batchsize = 16 # The size of the validation / testing batches\n",
    "epochs = 60 # The number of epochs to do\n",
    "learning_rate = 0.00005 # The learning rate to start at\n",
    "load_model = True # If a model should be requested to be loaded, or not\n",
    "save_model = True # If the model should be saved after testing, or not\n",
    "\n",
    "# The actual model variables\n",
    "model = None\n",
    "criterion = None\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "# Model file values. If \"None\", then they haven't loaded successfully\n",
    "md = None\n",
    "lr = None\n",
    "sch = None\n",
    "cri = None\n",
    "opt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs.\n",
      "Now the deivce is set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# By default, set to use the CPU\n",
    "deviceFlag = torch.device('cpu')\n",
    "\n",
    "# If a GPU is available, use it\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
    "    deviceFlag = torch.device('cuda:0') # Default to cuda 0, but can be changed.\n",
    "\n",
    "print(f'Now the deivce is set to {deviceFlag}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:43.165722100Z",
     "start_time": "2023-05-07T18:35:43.119409400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading and Transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    # Randomly rotate it 90 degrees\n",
    "    transforms.RandomRotation(90),\n",
    "    # Randomly sharpen the image\n",
    "    transforms.RandomAdjustSharpness(1.5, 0.5),\n",
    "    # Randomly crop an area of the flower of size 224x224\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    # Flip it horizontally, or don't\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Flip it vertically, or don't\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # Convert the image to a Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the Tensor values so that they're easier for the\n",
    "    # model to train from\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets of the Flower102 images\n",
    "train_dataset = datasets.Flowers102(root = './dataset', split = 'train', transform = training_transforms, download = True)\n",
    "valid_dataset = datasets.Flowers102(root = './dataset', split = 'val', transform = validation_transforms, download = True)\n",
    "test_dataset = datasets.Flowers102(root = './dataset', split = 'test', transform = testing_transforms, download = True)\n",
    "\n",
    "\n",
    "# Create the loaders for the datasets, to be used to train, validate and test the model\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = tr_batchsize,\n",
    "                                           shuffle = True)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                           batch_size = val_test_batchsize)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = val_test_batchsize)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:43.352688900Z",
     "start_time": "2023-05-07T18:35:43.145198400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Creating Model...\n",
      "Model created. Moving the Model to cuda...\n",
      "Moved the Model to cuda.\n",
      "\n",
      "Using model file from models/2023-05-07 19-26-48.437911 b16-e60-lr5e-05-model.pt\n"
     ]
    }
   ],
   "source": [
    "loaded_file = False\n",
    "\n",
    "file_name = \"ERROR\"\n",
    "# If going to load a model\n",
    "if load_model:\n",
    "    # First request the file name of a model\n",
    "    file_name = \"models/\" + input(\"Model to load: \")\n",
    "    # And try to load it\n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            file_data = torch.load(file_name)\n",
    "            md = file_data[\"model\"]\n",
    "            lr = file_data[\"learning_rate\"]\n",
    "            sch = file_data[\"scheduler\"]\n",
    "            cri = file_data[\"criterion\"]\n",
    "            opt = file_data[\"optimizer\"]\n",
    "            loaded_file = True\n",
    "            print(\"Model loaded.\")\n",
    "        except:\n",
    "            # If it fails, load nothing from the file\n",
    "            md = None\n",
    "            lr = learning_rate\n",
    "            sch = None\n",
    "            cri = None\n",
    "            opt = None\n",
    "            print(\"Model failed to load. Using default untrained Model.\")\n",
    "\n",
    "print(\"Creating Model...\")\n",
    "model = model_flower.FlowerModel()\n",
    "print(\"Model created. Moving the Model to \" + deviceFlag.type + \"...\")\n",
    "model.to(deviceFlag)\n",
    "print(\"Moved the Model to \" + deviceFlag.type + \".\")\n",
    "\n",
    "if loaded_file:\n",
    "    print(\"\\nUsing model file from \" + file_name)\n",
    "    model.load_state_dict(md)\n",
    "    learning_rate = lr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:51.163418600Z",
     "start_time": "2023-05-07T18:35:43.357692900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Loss Function and Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if cri is None:\n",
    "    criterion.load_state_dict(cri)\n",
    "\n",
    "# optimizer 1\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "if opt is None:\n",
    "    optimizer.load_state_dict(opt)\n",
    "\n",
    "# optimizer 2\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:51.179663400Z",
     "start_time": "2023-05-07T18:35:51.163418600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 500, learning_rate)\n",
    "if sch is None:\n",
    "    scheduler.load_state_dict(sch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:35:51.194819600Z",
     "start_time": "2023-05-07T18:35:51.177605300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] Epoch 1 completed on Batch 64 in 17.3738 seconds (17.3738 in total) \n",
      "\n",
      "[2/60] Batch: 100... Training Loss since last stepped validation: 1.4262... Validation Loss: 2.9158... Validation Accuracy: 0.3997\n",
      "[2/60] Epoch 2 completed on Batch 128 in 24.6245 seconds (41.9983 in total) \n",
      "\n",
      "[3/60] Epoch 3 completed on Batch 192 in 15.9865 seconds (57.9848 in total) \n",
      "\n",
      "[4/60] Batch: 200... Training Loss since last stepped validation: 1.3789... Validation Loss: 2.8003... Validation Accuracy: 0.4248\n",
      "[4/60] Epoch 4 completed on Batch 256 in 25.2342 seconds (83.2190 in total) \n",
      "\n",
      "[5/60] Batch: 300... Training Loss since last stepped validation: 1.3849... Validation Loss: 2.8587... Validation Accuracy: 0.4199\n",
      "[5/60] Epoch 5 completed on Batch 320 in 24.0994 seconds (107.3184 in total) \n",
      "\n",
      "[6/60] Epoch 6 completed on Batch 384 in 15.3398 seconds (122.6582 in total) \n",
      "\n",
      "[7/60] Batch: 400... Training Loss since last stepped validation: 1.3489... Validation Loss: 2.7470... Validation Accuracy: 0.4134\n",
      "[7/60] Epoch 7 completed on Batch 448 in 24.4555 seconds (147.1137 in total) \n",
      "\n",
      "[8/60] Batch: 500... Training Loss since last stepped validation: 1.2554... Validation Loss: 2.7545... Validation Accuracy: 0.4303\n",
      "[8/60] Epoch 8 completed on Batch 512 in 24.5734 seconds (171.6882 in total) \n",
      "\n",
      "[9/60] Epoch 9 completed on Batch 576 in 16.2844 seconds (187.9736 in total) \n",
      "\n",
      "[10/60] Batch: 600... Training Loss since last stepped validation: 1.2190... Validation Loss: 2.7242... Validation Accuracy: 0.4284\n",
      "[10/60] Epoch 10 completed on Batch 640 in 23.7002 seconds (211.6738 in total) \n",
      "\n",
      "[11/60] Batch: 700... Training Loss since last stepped validation: 1.1712... Validation Loss: 2.7186... Validation Accuracy: 0.4277\n",
      "[11/60] Epoch 11 completed on Batch 704 in 25.2959 seconds (236.9697 in total) \n",
      "\n",
      "[12/60] Epoch 12 completed on Batch 768 in 15.6936 seconds (252.6633 in total) \n",
      "\n",
      "[13/60] Batch: 800... Training Loss since last stepped validation: 1.1749... Validation Loss: 2.7278... Validation Accuracy: 0.4297\n",
      "[13/60] Epoch 13 completed on Batch 832 in 25.2776 seconds (277.9409 in total) \n",
      "\n",
      "[14/60] Epoch 14 completed on Batch 896 in 15.9852 seconds (293.9270 in total) \n",
      "\n",
      "[15/60] Batch: 900... Training Loss since last stepped validation: 1.1645... Validation Loss: 2.7342... Validation Accuracy: 0.4297\n",
      "[15/60] Epoch 15 completed on Batch 960 in 25.1986 seconds (319.1256 in total) \n",
      "\n",
      "[16/60] Batch: 1000... Training Loss since last stepped validation: 1.1605... Validation Loss: 2.7489... Validation Accuracy: 0.4326\n",
      "[16/60] Epoch 16 completed on Batch 1024 in 24.4939 seconds (343.6195 in total) \n",
      "\n",
      "[17/60] Epoch 17 completed on Batch 1088 in 16.2403 seconds (359.8597 in total) \n",
      "\n",
      "[18/60] Batch: 1100... Training Loss since last stepped validation: 1.1800... Validation Loss: 2.7354... Validation Accuracy: 0.4313\n",
      "[18/60] Epoch 18 completed on Batch 1152 in 25.1649 seconds (385.0247 in total) \n",
      "\n",
      "[19/60] Batch: 1200... Training Loss since last stepped validation: 1.1946... Validation Loss: 2.7374... Validation Accuracy: 0.4326\n",
      "[19/60] Epoch 19 completed on Batch 1216 in 25.0261 seconds (410.0518 in total) \n",
      "\n",
      "[20/60] Epoch 20 completed on Batch 1280 in 16.2768 seconds (426.3286 in total) \n",
      "\n",
      "[21/60] Batch: 1300... Training Loss since last stepped validation: 1.1669... Validation Loss: 2.7537... Validation Accuracy: 0.4316\n",
      "[21/60] Epoch 21 completed on Batch 1344 in 24.5875 seconds (450.9161 in total) \n",
      "\n",
      "[22/60] Batch: 1400... Training Loss since last stepped validation: 1.1571... Validation Loss: 2.7423... Validation Accuracy: 0.4326\n",
      "[22/60] Epoch 22 completed on Batch 1408 in 24.9006 seconds (475.8166 in total) \n",
      "\n",
      "[23/60] Epoch 23 completed on Batch 1472 in 16.0936 seconds (491.9102 in total) \n",
      "\n",
      "[24/60] Batch: 1500... Training Loss since last stepped validation: 1.2356... Validation Loss: 2.7607... Validation Accuracy: 0.4355\n",
      "[24/60] Epoch 24 completed on Batch 1536 in 24.6587 seconds (516.5689 in total) \n",
      "\n",
      "[25/60] Batch: 1600... Training Loss since last stepped validation: 1.1372... Validation Loss: 2.7794... Validation Accuracy: 0.4294\n",
      "[25/60] Epoch 25 completed on Batch 1600 in 25.0533 seconds (541.6223 in total) \n",
      "\n",
      "[26/60] Epoch 26 completed on Batch 1664 in 16.2577 seconds (557.8799 in total) \n",
      "\n",
      "[27/60] Batch: 1700... Training Loss since last stepped validation: 1.1913... Validation Loss: 2.7319... Validation Accuracy: 0.4323\n",
      "[27/60] Epoch 27 completed on Batch 1728 in 25.0768 seconds (582.9567 in total) \n",
      "\n",
      "[28/60] Epoch 28 completed on Batch 1792 in 16.3400 seconds (599.2967 in total) \n",
      "\n",
      "[29/60] Batch: 1800... Training Loss since last stepped validation: 1.1323... Validation Loss: 2.7343... Validation Accuracy: 0.4277\n",
      "[29/60] Epoch 29 completed on Batch 1856 in 26.5567 seconds (625.8535 in total) \n",
      "\n",
      "[30/60] Batch: 1900... Training Loss since last stepped validation: 1.1849... Validation Loss: 2.7391... Validation Accuracy: 0.4346\n",
      "[30/60] Epoch 30 completed on Batch 1920 in 24.6436 seconds (650.4970 in total) \n",
      "\n",
      "[31/60] Epoch 31 completed on Batch 1984 in 16.2987 seconds (666.7957 in total) \n",
      "\n",
      "[32/60] Batch: 2000... Training Loss since last stepped validation: 1.1643... Validation Loss: 2.7565... Validation Accuracy: 0.4277\n",
      "[32/60] Epoch 32 completed on Batch 2048 in 24.7823 seconds (691.5780 in total) \n",
      "\n",
      "[33/60] Batch: 2100... Training Loss since last stepped validation: 1.1512... Validation Loss: 2.7697... Validation Accuracy: 0.4277\n",
      "[33/60] Epoch 33 completed on Batch 2112 in 24.5134 seconds (716.0914 in total) \n",
      "\n",
      "[34/60] Epoch 34 completed on Batch 2176 in 16.0273 seconds (732.1187 in total) \n",
      "\n",
      "[35/60] Batch: 2200... Training Loss since last stepped validation: 1.1605... Validation Loss: 2.7812... Validation Accuracy: 0.4294\n",
      "[35/60] Epoch 35 completed on Batch 2240 in 24.6121 seconds (756.7308 in total) \n",
      "\n",
      "[36/60] Batch: 2300... Training Loss since last stepped validation: 1.1960... Validation Loss: 2.7294... Validation Accuracy: 0.4245\n",
      "[36/60] Epoch 36 completed on Batch 2304 in 24.8617 seconds (781.5925 in total) \n",
      "\n",
      "[37/60] Epoch 37 completed on Batch 2368 in 15.8060 seconds (797.3985 in total) \n",
      "\n",
      "[38/60] Batch: 2400... Training Loss since last stepped validation: 1.1716... Validation Loss: 2.7723... Validation Accuracy: 0.4284\n",
      "[38/60] Epoch 38 completed on Batch 2432 in 24.6274 seconds (822.0259 in total) \n",
      "\n",
      "[39/60] Epoch 39 completed on Batch 2496 in 16.0988 seconds (838.1247 in total) \n",
      "\n",
      "[40/60] Batch: 2500... Training Loss since last stepped validation: 1.1655... Validation Loss: 2.7447... Validation Accuracy: 0.4294\n",
      "[40/60] Epoch 40 completed on Batch 2560 in 25.2607 seconds (863.3854 in total) \n",
      "\n",
      "[41/60] Batch: 2600... Training Loss since last stepped validation: 1.1673... Validation Loss: 2.7653... Validation Accuracy: 0.4333\n",
      "[41/60] Epoch 41 completed on Batch 2624 in 25.2445 seconds (888.6299 in total) \n",
      "\n",
      "[42/60] Epoch 42 completed on Batch 2688 in 15.9790 seconds (904.6090 in total) \n",
      "\n",
      "[43/60] Batch: 2700... Training Loss since last stepped validation: 1.2308... Validation Loss: 2.7560... Validation Accuracy: 0.4316\n",
      "[43/60] Epoch 43 completed on Batch 2752 in 25.1343 seconds (929.7432 in total) \n",
      "\n",
      "[44/60] Batch: 2800... Training Loss since last stepped validation: 1.1918... Validation Loss: 2.7550... Validation Accuracy: 0.4346\n",
      "[44/60] Epoch 44 completed on Batch 2816 in 25.0274 seconds (954.7706 in total) \n",
      "\n",
      "[45/60] Epoch 45 completed on Batch 2880 in 15.0870 seconds (969.8576 in total) \n",
      "\n",
      "[46/60] Batch: 2900... Training Loss since last stepped validation: 1.1405... Validation Loss: 2.7446... Validation Accuracy: 0.4385\n",
      "[46/60] Epoch 46 completed on Batch 2944 in 24.4717 seconds (994.3293 in total) \n",
      "\n",
      "[47/60] Batch: 3000... Training Loss since last stepped validation: 1.1060... Validation Loss: 2.7504... Validation Accuracy: 0.4326\n",
      "[47/60] Epoch 47 completed on Batch 3008 in 24.8367 seconds (1019.1660 in total) \n",
      "\n",
      "[48/60] Epoch 48 completed on Batch 3072 in 15.8902 seconds (1035.0562 in total) \n",
      "\n",
      "[49/60] Batch: 3100... Training Loss since last stepped validation: 1.2337... Validation Loss: 2.7268... Validation Accuracy: 0.4274\n",
      "[49/60] Epoch 49 completed on Batch 3136 in 24.5130 seconds (1059.5693 in total) \n",
      "\n",
      "[50/60] Batch: 3200... Training Loss since last stepped validation: 1.1550... Validation Loss: 2.7626... Validation Accuracy: 0.4365\n",
      "[50/60] Epoch 50 completed on Batch 3200 in 24.8824 seconds (1084.4516 in total) \n",
      "\n",
      "[51/60] Epoch 51 completed on Batch 3264 in 16.3208 seconds (1100.7724 in total) \n",
      "\n",
      "[52/60] Batch: 3300... Training Loss since last stepped validation: 1.1332... Validation Loss: 2.7529... Validation Accuracy: 0.4303\n",
      "[52/60] Epoch 52 completed on Batch 3328 in 25.3144 seconds (1126.0868 in total) \n",
      "\n",
      "[53/60] Epoch 53 completed on Batch 3392 in 16.2873 seconds (1142.3741 in total) \n",
      "\n",
      "[54/60] Batch: 3400... Training Loss since last stepped validation: 1.1668... Validation Loss: 2.7477... Validation Accuracy: 0.4294\n",
      "[54/60] Epoch 54 completed on Batch 3456 in 24.4502 seconds (1166.8243 in total) \n",
      "\n",
      "[55/60] Batch: 3500... Training Loss since last stepped validation: 1.1399... Validation Loss: 2.7379... Validation Accuracy: 0.4346\n",
      "[55/60] Epoch 55 completed on Batch 3520 in 25.1661 seconds (1191.9905 in total) \n",
      "\n",
      "[56/60] Epoch 56 completed on Batch 3584 in 15.9986 seconds (1207.9890 in total) \n",
      "\n",
      "[57/60] Batch: 3600... Training Loss since last stepped validation: 1.1597... Validation Loss: 2.7721... Validation Accuracy: 0.4342\n",
      "[57/60] Epoch 57 completed on Batch 3648 in 24.9575 seconds (1232.9465 in total) \n",
      "\n",
      "[58/60] Batch: 3700... Training Loss since last stepped validation: 1.1488... Validation Loss: 2.7484... Validation Accuracy: 0.4346\n",
      "[58/60] Epoch 58 completed on Batch 3712 in 24.9712 seconds (1257.9191 in total) \n",
      "\n",
      "[59/60] Epoch 59 completed on Batch 3776 in 15.9970 seconds (1273.9162 in total) \n",
      "\n",
      "[60/60] Batch: 3800... Training Loss since last stepped validation: 1.1384... Validation Loss: 2.7706... Validation Accuracy: 0.4316\n",
      "[60/60] Epoch 60 completed on Batch 3840 in 24.9646 seconds (1298.8808 in total) \n",
      "\n",
      "\n",
      "Training Complete in 1298.8808 seconds.\n",
      "\n",
      "Validation of all 3840 Batches:\n",
      "Training Loss: 1.1939... Validation Loss: 2.7526... Validation Accuracy: 0.4352\n"
     ]
    },
    {
     "data": {
      "text/plain": "3840"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.train_classifier(model, train_loader, validate_loader, optimizer, criterion,\n",
    "                             optim_scheduler=scheduler, device_flag=deviceFlag, epochs=epochs,\n",
    "                             validate_steps=100, validate_stepped=True, validate_epoch=False,\n",
    "                             validate_end=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:57:38.887060700Z",
     "start_time": "2023-05-07T18:35:51.192785900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3960714340209961, 2431 / 6149 correctly predicted.\n"
     ]
    }
   ],
   "source": [
    "model_test.test_accuracy(model, test_loader, device_flag=deviceFlag)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:58:34.899509100Z",
     "start_time": "2023-05-07T18:57:38.890090900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    save_data = {\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"model\": model.state_dict(),\n",
    "        \"criterion\": criterion.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(save_data, \"models/\" + str(datetime.datetime.now()).replace(\":\",\"-\")\n",
    "               + f\" b{tr_batchsize}-e{epochs}-lr{learning_rate}\" \"-model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:58:36.937995Z",
     "start_time": "2023-05-07T18:58:34.903513800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Stop Run All here\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Stop Run All here\n",
    "assert False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T18:58:37.954708300Z",
     "start_time": "2023-05-07T18:58:36.941017500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reload imports in the case that they are changed\n",
    "from importlib import reload\n",
    "\n",
    "# If not loaded into cache yet, import them\n",
    "import model_flower\n",
    "import model_train\n",
    "import model_test\n",
    "\n",
    "reload(model_flower)\n",
    "reload(model_train)\n",
    "reload(model_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_model = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
