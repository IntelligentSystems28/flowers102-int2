{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.107696Z",
          "start_time": "2023-05-06T02:42:27.187630500Z"
        },
        "id": "W9QpA0RFXaeG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.121911300Z",
          "start_time": "2023-05-06T02:42:30.109702900Z"
        },
        "id": "hxEFQSjDXaeS"
      },
      "outputs": [],
      "source": [
        "tr_batchsize = 32\n",
        "val_test_batchsize = 16\n",
        "epochs = 60\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.166461200Z",
          "start_time": "2023-05-06T02:42:30.124922400Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utaO7saJXaeU",
        "outputId": "32e2fabc-b3a8-4c1f-be8a-48a55ee91cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPUs.\n",
            "Now the deivce is set to cuda:0\n"
          ]
        }
      ],
      "source": [
        "# By defalt, set device to the CPU\n",
        "deviceFlag = torch.device('cpu')\n",
        "\n",
        "# Default is CPU, but as long as GPU is avaliable, then use GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
        "    deviceFlag = torch.device('cuda:0') # Manually pick your cuda device. By default is 'cuda:0'\n",
        "\n",
        "print(f'Now the deivce is set to {deviceFlag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJHUEMBdXaeY"
      },
      "source": [
        "# VALIDATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:47:50.942069700Z",
          "start_time": "2023-05-06T02:47:50.925699Z"
        },
        "id": "gaO-2WtYXaec"
      },
      "outputs": [],
      "source": [
        "def validation(model, validate_loader, val_criterion):\n",
        "    \n",
        "    val_loss_running = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # a dataloader object is a generator of batches, each batch contain image & label separately\n",
        "    for images, labels in iter(validate_loader):\n",
        "        \n",
        "        # Send the data onto choosen device\n",
        "        images = images.to(deviceFlag)\n",
        "        labels = labels.to(deviceFlag)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        val_loss_running += val_criterion(output, labels).item() #*images.size(0) # .item() to get a scalar in Torch.tensor out\n",
        "        \n",
        "        probabilities = torch.exp(output) # as in the model we use the .LogSoftmax() output layer\n",
        "\n",
        "        equality = (labels.data == probabilities.max(dim=1)[1])\n",
        "        acc += equality.type(torch.FloatTensor).mean()\n",
        "        \n",
        "    return val_loss_running, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiTV0IYLXaef"
      },
      "source": [
        "# TRAINING + VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:47:51.869735900Z",
          "start_time": "2023-05-06T02:47:51.830156500Z"
        },
        "id": "7NAtQtN3Xaeg"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def train_eval(model, train_data_loader, validate_loader, train_criterion, optimizer, epoches, device_flag):\n",
        "\n",
        "    eval_itrs = len(train_data_loader)\n",
        "\n",
        "    # first setting the device used for training\n",
        "    model.to(device_flag)\n",
        "\n",
        "    batch_size = math.ceil(len(train_data_loader.dataset) / len(train_data_loader))\n",
        "\n",
        "    print(f'The training batchsize is {batch_size}.')\n",
        "    \n",
        "    # set the timer\n",
        "    since = time.time()\n",
        "\n",
        "    total_images = len(train_data_loader.dataset)\n",
        "\n",
        "    # ! THE EPOCH LOOP !\n",
        "    for e in range(epoches):\n",
        "        epoch_since = time.time()\n",
        "        epoch_text = f\"[{e + 1}/{epoches}]\"\n",
        "        itrs = 0\n",
        "\n",
        "        next_validation = eval_itrs\n",
        "        \n",
        "        # Set the model to the Train mode\n",
        "        # Tell the model to activate its Training behavior (turn-on the dropout & BN behaviors)\n",
        "        model.train()\n",
        "        \n",
        "        # re-initialize the running_loss to start every epoch\n",
        "        training_loss_running = 0\n",
        "\n",
        "        images_used = 0\n",
        "        \n",
        "        #  ! THE BATCH LOOP !\n",
        "        for inputs, labels in train_data_loader:\n",
        "            iter_text = \"\\r{} {} / {} - {:.2f}%.\".format(epoch_text,\n",
        "                                                         images_used, total_images,\n",
        "                                                         (images_used/total_images)*100)\n",
        "            print(\"\\r{} {} / {} - {:.2f}%.\"\n",
        "                  .format(epoch_text,\n",
        "                          images_used, total_images,\n",
        "                          (images_used/total_images)*100), end=\"\")\n",
        "            itrs += 1\n",
        "            # .to() method return a copy of the tensors on the targeted device\n",
        "            inputs = inputs.to(device_flag)\n",
        "            labels = labels.to(device_flag)\n",
        "            \n",
        "            # Clean the stored grads computed in the last iteration\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward Pass\n",
        "            # As model has been shipped to the targeted device, so the output is on that device too\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute Loss\n",
        "            train_loss = train_criterion(outputs, labels)\n",
        "            \n",
        "            # BackProp to compute the grads (stored in each tensor.grad() attributes) along the way\n",
        "            train_loss.backward()\n",
        "            \n",
        "            # Optimizer/Update params\n",
        "            optimizer.step()\n",
        "\n",
        "            #numeric ops, take the scalar out of the tensor by calling .item()\n",
        "            training_loss_running += train_loss.item()\n",
        "            \n",
        "            # ----------- Perform Validation (Evaluation) Every eval_itrs iterations ----------\n",
        "\n",
        "            if itrs >= next_validation:\n",
        "                # Move the next validation check\n",
        "                next_validation = itrs + eval_itrs\n",
        "\n",
        "                # Set the model to the Eval mode\n",
        "                model.eval()\n",
        "\n",
        "                # Turn-off gradient for validation to save memory & computation\n",
        "                with torch.no_grad():\n",
        "                    valid_text = f\"\\r{epoch_text} Validating...\"\n",
        "                    print(f\"\\r{valid_text}{' ' * (len(iter_text) - len(valid_text))}\", end=\"\")\n",
        "\n",
        "                    validation_loss, val_acc = validation(model, validate_loader, train_criterion)\n",
        "\n",
        "                    batch_text = f\"{epoch_text} Batch: {itrs}\"\n",
        "                    print(\"\\r{1}{2}\\n{0} | Train Loss: {3:.4f}\\n{0} | Valid Loss: {4:.4f}\\n{0} | Valid Accepted: {5:.10f}%\\n\".format(\n",
        "                        epoch_text, batch_text,\n",
        "                        \" \" * (len(valid_text) - len(batch_text)),\n",
        "                        training_loss_running / itrs,\n",
        "                        validation_loss,\n",
        "                        val_acc*100\n",
        "                    ))\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            images_used += inputs.size(0)\n",
        "                \n",
        "        end = time.time()\n",
        "\n",
        "        print('\\r{} Epoch took {:.4f} sec ({:.4f} total so far), Average Loss of Batches: {:.4f}\\n\\n'.format(\n",
        "            epoch_text,\n",
        "            end - epoch_since,\n",
        "            end - since,\n",
        "            training_loss_running / itrs\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLbMZ3bXaej"
      },
      "source": [
        "# TEST FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:47:53.776712900Z",
          "start_time": "2023-05-06T02:47:53.762821500Z"
        },
        "id": "YfORSy9GXael"
      },
      "outputs": [],
      "source": [
        "def test_acc(model, test_loader, device_flag):\n",
        "\n",
        "    # for testing, it is actually do validation on the test set\n",
        "    model.eval()\n",
        "\n",
        "    model.to(device_flag)\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    total_check = len(test_loader.dataset)\n",
        "\n",
        "    # In .eval() mode, set the context manager to turn-off grads\n",
        "    with torch.no_grad():\n",
        "        acc = 0\n",
        "\n",
        "        checked = 0\n",
        "\n",
        "        # iter() gives images and labels in batches\n",
        "        for inputs, labels in test_loader:\n",
        "            print(\"\\r{} / {} - {:.2f}%.\".format(checked, total_check, (checked / total_check) * 100), end=\"\")\n",
        "            \n",
        "            inputs = inputs.to(device_flag)\n",
        "            labels = labels.to(device_flag)\n",
        "\n",
        "            # Do a forward pass\n",
        "            output = model(inputs)\n",
        "            # convert the log likelihood to scalar\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "            acc += predicted.type(torch.FloatTensor).mean().item()\n",
        "\n",
        "            checked += labels.size(0)\n",
        "\n",
        "        end = time.time()\n",
        "        elapsed = end - since\n",
        "        print(\"\\r{} / {} predicted correctly. The accuracy of the model is {:.4f}%. ({:.2f}s taken)\".format(acc, checked, (acc / checked) * 100, elapsed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6nT9bLIXaen"
      },
      "source": [
        "# Checkpoint Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.277353200Z",
          "start_time": "2023-05-06T02:42:30.202427500Z"
        },
        "id": "IEQKpMbPXaeo"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, trainingdataset, saved_pth):\n",
        "    # set a new attr to the model object, which holds the class_to_idx conversion\n",
        "    model.class_to_idx = trainingdataset.class_to_idx\n",
        "    \n",
        "    # Chkpt is a dictionary, can be modified to hold anything you need in the furture\n",
        "    chkpt = {\n",
        "    'arch': 'vgg19',\n",
        "    'class_to_idx': model.class_to_idx,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "   # 'optimizer_state_dict': optimizer.state_dict()\n",
        "    }\n",
        "    \n",
        "    # Save with torch.save\n",
        "    torch.save(chkpt, saved_pth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfmcVV-cXaep"
      },
      "source": [
        "# Checkpoint Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.278354600Z",
          "start_time": "2023-05-06T02:42:30.220458800Z"
        },
        "id": "snpPLNZVXaeq"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(chkpt_path):\n",
        "    \n",
        "    chkpt = torch.load(chkpt_path)\n",
        "    \n",
        "    # After loading, the elements stored in the chkpt can be accesses as in a dict with key & value\n",
        "    if chkpt['arch'] == 'vgg19':\n",
        "        # Re-initial a new network arch\n",
        "        model = models.vgg19(pretrained = True)\n",
        "        \n",
        "        # Turn-off the .requires_grad attributes for all params in the feature extraction head\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "    \n",
        "    else:\n",
        "        print('------- Wrong Network Architecture is being used----------')\n",
        "    \n",
        "    model.class_to_idx = chkpt['class_to_idx']\n",
        "    \n",
        "    # Re-inital a new empty classisifer\n",
        "    \n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(25088, 4096)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('drop', nn.Dropout(p = 0.5)),\n",
        "        ('fc2', nn.Linear(4096, 102)),\n",
        "        ('output', nn.LogSoftmax(dim = 1))\n",
        "    ]))\n",
        "    \n",
        "    # Attach the classifer head\n",
        "    model.classifier = classifier\n",
        "    \n",
        "    # Load the params stored in the chkpt into the newly constructed empty model\n",
        "    # model.load_state_dict() is a built-in method of the models object\n",
        "    model.load_state_dict(chkpt['model_state_dict'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_685vsvbXaew"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:30.528548400Z",
          "start_time": "2023-05-06T02:42:30.282928200Z"
        },
        "id": "gqNirA-VXaew"
      },
      "outputs": [],
      "source": [
        "training_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "testing_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the datasets with torchvision.datasets.ImageFolder object\n",
        "train_dataset = datasets.Flowers102(root = './dataset', split = 'train', transform = training_transforms, download = True)\n",
        "valid_dataset = datasets.Flowers102(root = './dataset', split = 'val', transform = validation_transforms, download = True)\n",
        "test_dataset = datasets.Flowers102(root = './dataset', split = 'test', transform = testing_transforms, download = True)\n",
        "\n",
        "# Instantiate loader objects to facilitate processing\n",
        "\n",
        "\n",
        "# Define the torch.utils.data.DataLoader() object with the ImageFolder object\n",
        "# Dataloader is a generator to read from ImageFolder and generate them into batch-by-batch\n",
        "# Only shuffle during trianing, validation and testing no shuffles\n",
        "# the batchsize for training and tesitng no need to be the same\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = tr_batchsize,\n",
        "                                           shuffle = True)\n",
        "\n",
        "validate_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
        "                                           batch_size = val_test_batchsize)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = val_test_batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:56.213969300Z",
          "start_time": "2023-05-06T02:42:56.192640200Z"
        },
        "id": "hUdVmQ1-ajZb"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(32), \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU() \n",
        "            # nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            # # nn.BatchNorm2d(64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        # self.layer2 = nn.Sequential(\n",
        "            \n",
        "        # self.layer3 = nn.Sequential(\n",
        "            \n",
        "        # self.layer4 = nn.Sequential(\n",
        "            \n",
        "        # self.layer5 = nn.Sequential(\n",
        "        #     nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(128),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer6 = nn.Sequential(\n",
        "        #     nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(128),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer7 = nn.Sequential(\n",
        "        #     nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(256),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        # self.layer8 = nn.Sequential(\n",
        "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer9 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer10 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        # self.layer11 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer12 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU())\n",
        "        # self.layer13 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "        #     # nn.BatchNorm2d(512),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # nn.Dropout(0.5),\n",
        "            # first linear must be image size * image size * last Conv2d out channel\n",
        "            nn.Linear(7*7*4096, 1024),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "        # self.fc1 = nn.Sequential(\n",
        "\n",
        "        # self.fc2= nn.Sequential(\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        # out = self.layer2(out)\n",
        "        # out = self.layer3(out)\n",
        "        # out = self.layer4(out)\n",
        "        # out = self.layer5(out)\n",
        "        # out = self.layer6(out)\n",
        "        # out = self.layer7(out)\n",
        "        # out = self.layer8(out)\n",
        "        # out = self.layer9(out)\n",
        "        # out = self.layer10(out)\n",
        "        # out = self.layer11(out)\n",
        "        # out = self.layer12(out)\n",
        "        # out = self.layer13(out)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out = self.fc1(out)\n",
        "        # out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:58.741422200Z",
          "start_time": "2023-05-06T02:42:57.999495100Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWmJtZd2Xaex",
        "outputId": "675ce95a-9a6c-414e-da1c-aa6d7a4bcf50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG16(\n",
              "  (layer1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=200704, out_features=1024, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=512, out_features=102, bias=True)\n",
              "    (6): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "model = VGG16()\n",
        "model.to(deviceFlag)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:42:59.456119100Z",
          "start_time": "2023-05-06T02:42:59.429376100Z"
        },
        "id": "DclH6p0JXaey"
      },
      "outputs": [],
      "source": [
        "# for params in model.parameters():\n",
        "#     params.requries_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuQ_KXFXae0"
      },
      "source": [
        "# Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:45:27.688621300Z",
          "start_time": "2023-05-06T02:45:27.668568900Z"
        },
        "id": "kgDNGJAxXae0"
      },
      "outputs": [],
      "source": [
        "# Negative Log Likelihood Loss\n",
        "# criterion = nn.NLLLoss()\n",
        "\n",
        "# Cross Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer 1\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "# optimizer 2\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier():\n",
        "      steps = 0\n",
        "      print_every = 16\n",
        "\n",
        "      model.to(deviceFlag)\n",
        "\n",
        "      for e in range(epochs):\n",
        "        \n",
        "          model.train()\n",
        "    \n",
        "          running_loss = 0\n",
        "    \n",
        "          for images, labels in iter(train_loader):\n",
        "        \n",
        "              steps += 1\n",
        "        \n",
        "              images, labels = images.to(deviceFlag), labels.to(deviceFlag)\n",
        "        \n",
        "              optimizer.zero_grad()\n",
        "        \n",
        "              output = model.forward(images)\n",
        "              loss = criterion(output, labels)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "        \n",
        "              running_loss += loss.item()\n",
        "        \n",
        "              if steps % print_every == 0:\n",
        "                \n",
        "                  model.eval()\n",
        "                \n",
        "                  # Turn off gradients for validation, saves memory and computations\n",
        "                  with torch.no_grad():\n",
        "                      validation_loss, accuracy = validation(model, validate_loader, criterion)\n",
        "            \n",
        "                  print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                        \"Training Loss: {:.3f}... \".format(running_loss/print_every),\n",
        "                        \"Validation Loss: {:.3f}... \".format(validation_loss/len(validate_loader)),\n",
        "                        \"Validation Accuracy: {:.3f}\".format(accuracy/len(validate_loader)))\n",
        "            \n",
        "                  running_loss = 0\n",
        "                  model.train()\n",
        "train_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C7lTL2ajQJm",
        "outputId": "b568fa12-d25f-447e-934b-0a4ef507b577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/60..  Training Loss: 5.121...  Validation Loss: 4.625...  Validation Accuracy: 0.010\n",
            "Epoch: 1/60..  Training Loss: 4.627...  Validation Loss: 4.624...  Validation Accuracy: 0.010\n",
            "Epoch: 2/60..  Training Loss: 4.610...  Validation Loss: 4.562...  Validation Accuracy: 0.011\n",
            "Epoch: 2/60..  Training Loss: 4.531...  Validation Loss: 4.453...  Validation Accuracy: 0.022\n",
            "Epoch: 3/60..  Training Loss: 4.430...  Validation Loss: 4.380...  Validation Accuracy: 0.019\n",
            "Epoch: 3/60..  Training Loss: 4.361...  Validation Loss: 4.225...  Validation Accuracy: 0.027\n",
            "Epoch: 4/60..  Training Loss: 4.256...  Validation Loss: 4.154...  Validation Accuracy: 0.037\n",
            "Epoch: 4/60..  Training Loss: 4.184...  Validation Loss: 4.052...  Validation Accuracy: 0.041\n",
            "Epoch: 5/60..  Training Loss: 4.046...  Validation Loss: 3.943...  Validation Accuracy: 0.047\n",
            "Epoch: 5/60..  Training Loss: 4.093...  Validation Loss: 4.042...  Validation Accuracy: 0.035\n",
            "Epoch: 6/60..  Training Loss: 3.983...  Validation Loss: 4.047...  Validation Accuracy: 0.040\n",
            "Epoch: 6/60..  Training Loss: 3.980...  Validation Loss: 3.897...  Validation Accuracy: 0.064\n",
            "Epoch: 7/60..  Training Loss: 3.864...  Validation Loss: 3.825...  Validation Accuracy: 0.064\n",
            "Epoch: 7/60..  Training Loss: 3.856...  Validation Loss: 3.785...  Validation Accuracy: 0.083\n",
            "Epoch: 8/60..  Training Loss: 3.762...  Validation Loss: 3.853...  Validation Accuracy: 0.063\n",
            "Epoch: 8/60..  Training Loss: 3.816...  Validation Loss: 3.692...  Validation Accuracy: 0.083\n",
            "Epoch: 9/60..  Training Loss: 3.653...  Validation Loss: 3.720...  Validation Accuracy: 0.093\n",
            "Epoch: 9/60..  Training Loss: 3.602...  Validation Loss: 3.579...  Validation Accuracy: 0.097\n",
            "Epoch: 10/60..  Training Loss: 3.483...  Validation Loss: 3.630...  Validation Accuracy: 0.108\n",
            "Epoch: 10/60..  Training Loss: 3.595...  Validation Loss: 3.516...  Validation Accuracy: 0.104\n",
            "Epoch: 11/60..  Training Loss: 3.522...  Validation Loss: 3.571...  Validation Accuracy: 0.095\n",
            "Epoch: 11/60..  Training Loss: 3.626...  Validation Loss: 3.475...  Validation Accuracy: 0.136\n",
            "Epoch: 12/60..  Training Loss: 3.402...  Validation Loss: 3.462...  Validation Accuracy: 0.149\n",
            "Epoch: 12/60..  Training Loss: 3.496...  Validation Loss: 3.402...  Validation Accuracy: 0.153\n",
            "Epoch: 13/60..  Training Loss: 3.343...  Validation Loss: 3.430...  Validation Accuracy: 0.130\n",
            "Epoch: 13/60..  Training Loss: 3.419...  Validation Loss: 3.476...  Validation Accuracy: 0.128\n",
            "Epoch: 14/60..  Training Loss: 3.322...  Validation Loss: 3.408...  Validation Accuracy: 0.141\n",
            "Epoch: 14/60..  Training Loss: 3.325...  Validation Loss: 3.389...  Validation Accuracy: 0.135\n",
            "Epoch: 15/60..  Training Loss: 3.313...  Validation Loss: 3.589...  Validation Accuracy: 0.118\n",
            "Epoch: 15/60..  Training Loss: 3.211...  Validation Loss: 3.408...  Validation Accuracy: 0.164\n",
            "Epoch: 16/60..  Training Loss: 3.227...  Validation Loss: 3.323...  Validation Accuracy: 0.144\n",
            "Epoch: 16/60..  Training Loss: 3.334...  Validation Loss: 3.314...  Validation Accuracy: 0.160\n",
            "Epoch: 17/60..  Training Loss: 3.194...  Validation Loss: 3.382...  Validation Accuracy: 0.144\n",
            "Epoch: 17/60..  Training Loss: 3.291...  Validation Loss: 3.312...  Validation Accuracy: 0.133\n",
            "Epoch: 18/60..  Training Loss: 3.063...  Validation Loss: 3.291...  Validation Accuracy: 0.157\n",
            "Epoch: 18/60..  Training Loss: 3.149...  Validation Loss: 3.317...  Validation Accuracy: 0.182\n",
            "Epoch: 19/60..  Training Loss: 2.961...  Validation Loss: 3.299...  Validation Accuracy: 0.176\n",
            "Epoch: 19/60..  Training Loss: 3.216...  Validation Loss: 3.211...  Validation Accuracy: 0.181\n",
            "Epoch: 20/60..  Training Loss: 3.056...  Validation Loss: 3.200...  Validation Accuracy: 0.175\n",
            "Epoch: 20/60..  Training Loss: 2.928...  Validation Loss: 3.194...  Validation Accuracy: 0.187\n",
            "Epoch: 21/60..  Training Loss: 2.914...  Validation Loss: 3.293...  Validation Accuracy: 0.176\n",
            "Epoch: 21/60..  Training Loss: 3.126...  Validation Loss: 3.203...  Validation Accuracy: 0.184\n",
            "Epoch: 22/60..  Training Loss: 2.852...  Validation Loss: 3.148...  Validation Accuracy: 0.216\n",
            "Epoch: 22/60..  Training Loss: 3.017...  Validation Loss: 3.120...  Validation Accuracy: 0.209\n",
            "Epoch: 23/60..  Training Loss: 2.865...  Validation Loss: 3.201...  Validation Accuracy: 0.201\n",
            "Epoch: 23/60..  Training Loss: 2.900...  Validation Loss: 3.117...  Validation Accuracy: 0.204\n",
            "Epoch: 24/60..  Training Loss: 2.832...  Validation Loss: 3.139...  Validation Accuracy: 0.206\n",
            "Epoch: 24/60..  Training Loss: 2.903...  Validation Loss: 3.088...  Validation Accuracy: 0.210\n",
            "Epoch: 25/60..  Training Loss: 2.723...  Validation Loss: 3.127...  Validation Accuracy: 0.227\n",
            "Epoch: 25/60..  Training Loss: 2.840...  Validation Loss: 3.084...  Validation Accuracy: 0.222\n",
            "Epoch: 26/60..  Training Loss: 2.783...  Validation Loss: 3.089...  Validation Accuracy: 0.229\n",
            "Epoch: 26/60..  Training Loss: 2.802...  Validation Loss: 3.188...  Validation Accuracy: 0.226\n",
            "Epoch: 27/60..  Training Loss: 2.682...  Validation Loss: 3.124...  Validation Accuracy: 0.222\n",
            "Epoch: 27/60..  Training Loss: 2.829...  Validation Loss: 3.120...  Validation Accuracy: 0.228\n",
            "Epoch: 28/60..  Training Loss: 2.692...  Validation Loss: 3.058...  Validation Accuracy: 0.254\n",
            "Epoch: 28/60..  Training Loss: 2.740...  Validation Loss: 3.022...  Validation Accuracy: 0.225\n",
            "Epoch: 29/60..  Training Loss: 2.524...  Validation Loss: 3.205...  Validation Accuracy: 0.220\n",
            "Epoch: 29/60..  Training Loss: 2.724...  Validation Loss: 3.019...  Validation Accuracy: 0.261\n",
            "Epoch: 30/60..  Training Loss: 2.479...  Validation Loss: 2.952...  Validation Accuracy: 0.270\n",
            "Epoch: 30/60..  Training Loss: 2.516...  Validation Loss: 2.968...  Validation Accuracy: 0.283\n",
            "Epoch: 31/60..  Training Loss: 2.464...  Validation Loss: 2.993...  Validation Accuracy: 0.278\n",
            "Epoch: 31/60..  Training Loss: 2.621...  Validation Loss: 3.057...  Validation Accuracy: 0.240\n",
            "Epoch: 32/60..  Training Loss: 2.528...  Validation Loss: 3.125...  Validation Accuracy: 0.249\n",
            "Epoch: 32/60..  Training Loss: 2.570...  Validation Loss: 3.120...  Validation Accuracy: 0.246\n",
            "Epoch: 33/60..  Training Loss: 2.424...  Validation Loss: 2.985...  Validation Accuracy: 0.238\n",
            "Epoch: 33/60..  Training Loss: 2.445...  Validation Loss: 3.266...  Validation Accuracy: 0.241\n",
            "Epoch: 34/60..  Training Loss: 2.413...  Validation Loss: 3.123...  Validation Accuracy: 0.250\n",
            "Epoch: 34/60..  Training Loss: 2.523...  Validation Loss: 3.012...  Validation Accuracy: 0.278\n",
            "Epoch: 35/60..  Training Loss: 2.345...  Validation Loss: 3.328...  Validation Accuracy: 0.259\n",
            "Epoch: 35/60..  Training Loss: 2.531...  Validation Loss: 2.989...  Validation Accuracy: 0.272\n",
            "Epoch: 36/60..  Training Loss: 2.338...  Validation Loss: 3.341...  Validation Accuracy: 0.260\n",
            "Epoch: 36/60..  Training Loss: 2.489...  Validation Loss: 3.006...  Validation Accuracy: 0.282\n",
            "Epoch: 37/60..  Training Loss: 2.284...  Validation Loss: 3.007...  Validation Accuracy: 0.312\n",
            "Epoch: 37/60..  Training Loss: 2.485...  Validation Loss: 2.914...  Validation Accuracy: 0.284\n",
            "Epoch: 38/60..  Training Loss: 2.299...  Validation Loss: 2.965...  Validation Accuracy: 0.277\n",
            "Epoch: 38/60..  Training Loss: 2.275...  Validation Loss: 3.045...  Validation Accuracy: 0.273\n",
            "Epoch: 39/60..  Training Loss: 2.267...  Validation Loss: 2.991...  Validation Accuracy: 0.271\n",
            "Epoch: 39/60..  Training Loss: 2.310...  Validation Loss: 3.132...  Validation Accuracy: 0.274\n",
            "Epoch: 40/60..  Training Loss: 2.170...  Validation Loss: 3.149...  Validation Accuracy: 0.263\n",
            "Epoch: 40/60..  Training Loss: 2.279...  Validation Loss: 3.071...  Validation Accuracy: 0.296\n",
            "Epoch: 41/60..  Training Loss: 2.249...  Validation Loss: 3.074...  Validation Accuracy: 0.283\n",
            "Epoch: 41/60..  Training Loss: 2.299...  Validation Loss: 3.053...  Validation Accuracy: 0.296\n",
            "Epoch: 42/60..  Training Loss: 2.043...  Validation Loss: 3.231...  Validation Accuracy: 0.287\n",
            "Epoch: 42/60..  Training Loss: 2.184...  Validation Loss: 3.043...  Validation Accuracy: 0.275\n",
            "Epoch: 43/60..  Training Loss: 2.027...  Validation Loss: 3.145...  Validation Accuracy: 0.303\n",
            "Epoch: 43/60..  Training Loss: 2.253...  Validation Loss: 2.966...  Validation Accuracy: 0.304\n",
            "Epoch: 44/60..  Training Loss: 2.018...  Validation Loss: 3.238...  Validation Accuracy: 0.289\n",
            "Epoch: 44/60..  Training Loss: 2.111...  Validation Loss: 3.148...  Validation Accuracy: 0.286\n",
            "Epoch: 45/60..  Training Loss: 1.992...  Validation Loss: 3.083...  Validation Accuracy: 0.303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T02:58:11.279947600Z",
          "start_time": "2023-05-06T02:56:48.409257100Z"
        },
        "id": "GfHUyA2BXae1"
      },
      "outputs": [],
      "source": [
        "# train_eval(model, train_loader, validate_loader, criterion, optimizer, epochs, deviceFlag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-06T03:04:16.096720400Z",
          "start_time": "2023-05-06T03:02:48.942108Z"
        },
        "id": "0phs-jgaRoYt"
      },
      "outputs": [],
      "source": [
        "# test_acc(model, test_loader, deviceFlag)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_accuracy(model, test_loader):\n",
        "\n",
        "    # Do validation on the test set\n",
        "    model.eval()\n",
        "    model.to(deviceFlag)\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        accuracy = 0\n",
        "    \n",
        "        for images, labels in iter(test_loader):\n",
        "    \n",
        "            images, labels = images.to(deviceFlag), labels.to(deviceFlag)\n",
        "    \n",
        "            output = model.forward(images)\n",
        "\n",
        "            probabilities = torch.exp(output)\n",
        "        \n",
        "            equality = (labels.data == probabilities.max(dim=1)[1])\n",
        "        \n",
        "            accuracy += equality.type(torch.FloatTensor).mean()\n",
        "        \n",
        "        print(\"Test Accuracy: {}\".format(accuracy/len(test_loader)))    \n",
        "        \n",
        "        \n",
        "test_accuracy(model, test_loader)\n"
      ],
      "metadata": {
        "id": "JSnssQL0j-AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLS4GGfxtAex"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"2023-05-06--03-00-model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ewf6DqpsY19"
      },
      "outputs": [],
      "source": [
        "# total_step = len(train_loader)\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     for i, (images, labels) in enumerate(train_loader):  \n",
        "#         # Move tensors to the configured device\n",
        "#         images = images.to(deviceFlag)\n",
        "#         labels = labels.to(deviceFlag)\n",
        "        \n",
        "#         # Forward pass\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "#                    .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "#     # Validation\n",
        "#     with torch.no_grad():\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "#         for images, labels in validate_loader:\n",
        "#             images = images.to(deviceFlag)\n",
        "#             labels = labels.to(deviceFlag)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#             del images, labels, outputs\n",
        "    \n",
        "#         print('Accuracy of the network on the {} validation images: {} %'.format(total, 100 * correct / total)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOkitkfYsZHM"
      },
      "outputs": [],
      "source": [
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for images, labels in test_loader:\n",
        "#         images = images.to(deviceFlag)\n",
        "#         labels = labels.to(deviceFlag)\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         del images, labels, outputs\n",
        "\n",
        "#     print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}