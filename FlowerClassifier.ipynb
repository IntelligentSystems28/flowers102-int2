{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W9QpA0RFXaeG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hxEFQSjDXaeS"
      },
      "outputs": [],
      "source": [
        "tr_batchsize = 64\n",
        "val_test_batchsize = 32\n",
        "epochs = 60\n",
        "lr = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utaO7saJXaeU",
        "outputId": "491a0951-62b9-4d80-cad6-87726c2bdfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 GPUs.\n",
            "Now the deivce is set to cuda:0\n"
          ]
        }
      ],
      "source": [
        "# By defalt, set device to the CPU\n",
        "deviceFlag = torch.device('cpu')\n",
        "\n",
        "# Defalut is CPU, but as long as GPU is avaliable, then use GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
        "    deviceFlag = torch.device('cuda:0') # Manually pick your cuda device. By default is 'cuda:0'\n",
        "\n",
        "print(f'Now the deivce is set to {deviceFlag}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJHUEMBdXaeY"
      },
      "source": [
        "# VALIDATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gaO-2WtYXaec"
      },
      "outputs": [],
      "source": [
        "def validation(model, validateloader, ValCriterion):\n",
        "    \n",
        "    val_loss_running = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # a dataloader object is a generator of batches, each batch contain image & label separately\n",
        "    for images, labels in iter(validateloader):\n",
        "        \n",
        "        # Send the data onto choosen device\n",
        "        images = images.to(deviceFlag)\n",
        "        labels = labels.to(deviceFlag)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        val_loss_running += ValCriterion(output, labels).item() # .item() to get a scalar in Torch.tensor out\n",
        "        \n",
        "        output = torch.exp(output) # as in the model we use the .LogSoftmax() output layer\n",
        "        \n",
        "        equals = (labels.data == output.max(dim = 1)[1])\n",
        "        acc += equals.float().mean().item() # .flaot() is to transfer the tensor.cuda.float type onto cpu mode\n",
        "        \n",
        "    return val_loss_running / len(validateloader), acc / len(validateloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiTV0IYLXaef"
      },
      "source": [
        "# TRAINING + VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7NAtQtN3Xaeg"
      },
      "outputs": [],
      "source": [
        "def train_eval(model, traindataloader, validateloader, TrCriterion, optimizer, epochs, deviceFlag_train):\n",
        "    \n",
        "    itrs = 0\n",
        "    eval_itrs = 40\n",
        "    \n",
        "    # first setting the device used for training\n",
        "    model.to(deviceFlag_train)\n",
        "    \n",
        "    print(f'The training batchsize is {tr_batchsize}.')\n",
        "    \n",
        "    # set the timer\n",
        "    since = time.time()\n",
        "\n",
        "    # ! THE EPOCH LOOP !\n",
        "    for e in range(epochs):        \n",
        "        itrs = 0\n",
        "        \n",
        "        # Set the model to the Train mode\n",
        "        # Tell the model to activate its Training behavior (turn-on the dropout & BN behaviors)\n",
        "        model.train()\n",
        "        \n",
        "        # re-initialize the running_loss to start every epoch\n",
        "        training_loss_running = 0\n",
        "        \n",
        "        #  ! THE BATCH LOOP !\n",
        "        for inputs, labels in iter(traindataloader):            \n",
        "            itrs += 1\n",
        "            # .to() method return a copy of the tensors on the targeted device\n",
        "            inputs = inputs.to(deviceFlag_train)\n",
        "            labels = labels.to(deviceFlag_train)\n",
        "            \n",
        "            # Clean the stored grads computed in the last iteration\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward Pass\n",
        "            # As model has been shipped to the targeted device, so the output is on that device too\n",
        "            outputs = model.forward(inputs)\n",
        "            \n",
        "            # Compute Loss\n",
        "            train_loss = TrCriterion(outputs, labels)\n",
        "            \n",
        "            # BackProp to compute the grads (stored in each tensor.grad() attributes) along the way\n",
        "            train_loss.backward()\n",
        "            \n",
        "            # Optimizer/Update params\n",
        "            optimizer.step()\n",
        "            \n",
        "            training_loss_running += train_loss.item() #numeric ops, take the scalar out of the tensor by calling .item()\n",
        "            \n",
        "            # ----------- Perform Validation (Evaluation) Every eval_itrs iterations ---------- #\n",
        "            if itrs % eval_itrs == 0:\n",
        "                \n",
        "                # Set the model to the Eval mode\n",
        "                model.eval()\n",
        "                \n",
        "                # Turn-off gradient for validation to save memory & computation\n",
        "                with torch.no_grad():\n",
        "                    validation_loss, val_acc = validation(model, validateloader, TrCriterion)\n",
        "                \n",
        "                display = f'Epoch: {e + 1}/{epochs}, itrs: {itrs}, '\n",
        "                display += f'Train_loss: {round(training_loss_running / eval_itrs, 4)}, '\n",
        "                display += f'Valid_loss: {round(validation_loss, 4)}, '\n",
        "                display += f'Valid_Acc: {round(val_acc, 4)}'\n",
        "                print(display)\n",
        "                \n",
        "                training_loss_running = 0\n",
        "                model.train()\n",
        "                \n",
        "        end = time.time()\n",
        "        elapsed = end - since\n",
        "        print(f'Epoch {e + 1} takes {round(elapsed, 4)} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLbMZ3bXaej"
      },
      "source": [
        "# TEST FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YfORSy9GXael"
      },
      "outputs": [],
      "source": [
        "def test_acc(model, test_loader, deviceFlag_test):\n",
        "\n",
        "    # for testing, it is actually do validation on the test set\n",
        "    model.eval()\n",
        "\n",
        "    model.to(deviceFlag_test)\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    # In .eval() mode, set the context manager to turn-off grads\n",
        "    with torch.no_grad():\n",
        "        acc = 0\n",
        "\n",
        "        # iter() gives images and labels in batches\n",
        "        for inputs, labels in iter(test_loader):\n",
        "            \n",
        "            inputs = inputs.to(deviceFlag_test)\n",
        "            labels = labels.to(deviceFlag_test)\n",
        "\n",
        "            # Do a forward pass\n",
        "            output = model.forward(inputs)\n",
        "            # convert the log likelihood to scalar\n",
        "            prob = torch.exp(output)\n",
        "\n",
        "            equals = (labels.data == prob.max(dim = 1)[1])\n",
        "\n",
        "            acc += equals.type(torch.FloatTensor).mean().item()\n",
        "\n",
        "        end = time.time()\n",
        "        elapsed = end - since\n",
        "\n",
        "        print(f'Test_acc: {round(acc, 4)}, tiem_spent: {round(elapsed, 2)} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6nT9bLIXaen"
      },
      "source": [
        "# Checkpoint Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IEQKpMbPXaeo"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, trainingdataset, saved_pth):\n",
        "    # set a new attr to the model object, which holds the class_to_idx conversion\n",
        "    model.class_to_idx = trainingdataset.class_to_idx\n",
        "    \n",
        "    # Chkpt is a dictionary, can be modified to hold anything you need in the furture\n",
        "    chkpt = {\n",
        "    'arch': 'vgg19',\n",
        "    'class_to_idx': model.class_to_idx,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "   # 'optimizer_state_dict': optimizer.state_dict()\n",
        "    }\n",
        "    \n",
        "    # Save with torch.save\n",
        "    torch.save(chkpt, saved_pth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfmcVV-cXaep"
      },
      "source": [
        "# Checkpoint Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "snpPLNZVXaeq"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(chkpt_path):\n",
        "    \n",
        "    chkpt = torch.load(chkpt_path)\n",
        "    \n",
        "    # After loading, the elements stored in the chkpt can be accesses as in a dict with key & value\n",
        "    if chkpt['arch'] == 'vgg19':\n",
        "        # Re-initial a new network arch\n",
        "        model = models.vgg19(pretrained = True)\n",
        "        \n",
        "        # Turn-off the .requires_grad attributes for all params in the feature extraction head\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "    \n",
        "    else:\n",
        "        print('------- Wrong Network Architecture is being used----------')\n",
        "    \n",
        "    model.class_to_idx = chkpt['class_to_idx']\n",
        "    \n",
        "    # Re-inital a new empty classisifer\n",
        "    \n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(25088, 4096)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('drop', nn.Dropout(p = 0.5)),\n",
        "        ('fc2', nn.Linear(4096, 102)),\n",
        "        ('output', nn.LogSoftmax(dim = 1))\n",
        "    ]))\n",
        "    \n",
        "    # Attach the classifer head\n",
        "    model.classifier = classifier\n",
        "    \n",
        "    # Load the params stored in the chkpt into the newly constructed empty model\n",
        "    # model.load_state_dict() is a built-in method of the models object\n",
        "    model.load_state_dict(chkpt['model_state_dict'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6Mu3YTSXaeq"
      },
      "source": [
        "# Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-gtI6bd_Xaer"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img_pth):\n",
        "    '''\n",
        "    Input a PIL image, output a numpy array with axes transposed to [Ch, H, W]\n",
        "    '''\n",
        "    pil_image = Image.open(img_pth)\n",
        "    \n",
        "    # -------- Resize with Aspect Ratio maintained--------- #\n",
        "    # First fixing the short axes\n",
        "    if pil_image.size[0] > pil_image.size[1]:\n",
        "        pil_image.thumbnail((10000000, 256))\n",
        "    else:\n",
        "        pil_image.thumbnail((256, 100000000))\n",
        "    \n",
        "    # ---------Crop----------- #\n",
        "    left_margin = (pil_image.width - 224) / 2\n",
        "    bottom_margin = (pil_image.height - 224) / 2\n",
        "    right_margin = left_margin + 224\n",
        "    top_margin = bottom_margin + 224\n",
        "    \n",
        "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
        "    \n",
        "    # --------- Convert to np then Normalize ----------- #\n",
        "    np_image = np.array(pil_image) / 255\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    np_image = (np_image -mean) / std\n",
        "    \n",
        "    # --------- Transpose to fit PyTorch Axes ----------#\n",
        "    np_image = np_image.transpose([2, 0, 1])\n",
        "    \n",
        "    return np_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfwwMY6yXaes"
      },
      "source": [
        "# Image Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "27c6Dd1aXaes"
      },
      "outputs": [],
      "source": [
        "def imshow(pt_image, ax = None, title = None):\n",
        "    '''\n",
        "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
        "    Convert it back to [H, W, Ch], \n",
        "    Undo the preprocessing,\n",
        "    then display it on a grid\n",
        "    '''\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    \n",
        "    # --------- Transpose ----------- #\n",
        "    plt_image = pt_image.transpose((1, 2, 0))\n",
        "    \n",
        "    # --------- Undo the preprocessing --------- #\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    plt_image = plt_image * std + mean\n",
        "    \n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "        \n",
        "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
        "    plt_image = np.clip(plt_image, 0, 1)\n",
        "    \n",
        "    # this imshow is a function defined in the plt module\n",
        "    ax.imshow(plt_image)\n",
        "    \n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQbQS2NZXaeu"
      },
      "source": [
        "# Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sDJeNpESXaev"
      },
      "outputs": [],
      "source": [
        "def predict(img_pth, model, trainingdataset, topk):\n",
        "    '''\n",
        "    1. input a single img;\n",
        "    2. forward pass on a model;\n",
        "    3. use tensor.topk(k) to return the highest k probs and the correspodniung class idx;\n",
        "    4. convert the idx to class names using the name_to_idx conversion.\n",
        "    '''\n",
        "    np_img = image_preprocessing(img_pth)\n",
        "    \n",
        "    # Convert np_img to PT tensor and send to GPU\n",
        "    pt_img = torch.from_numpy(np_img).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    # Unsqueeze to get shape of tensor from [Ch, H, W] to [Batch, Ch, H, W]\n",
        "    pt_img = pt_img.unsqueeze(0)\n",
        "\n",
        "    # Run the model to predict\n",
        "    output = model.forward(pt_img)\n",
        "    \n",
        "    probs = torch.exp(output)\n",
        "    \n",
        "    # Pick out the topk from all classes \n",
        "    top_probs, top_indices = probs.topk(topk)\n",
        "    \n",
        "    # Convert to list on CPU without grads\n",
        "    top_probs = top_probs.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
        "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0]\n",
        "    \n",
        "    # Invert the class_to_idx dict to a idx_to_class dict\n",
        "    idx_to_class = {value: key for key, value in trainingdataset.class_to_idx.items()}\n",
        "    \n",
        "    top_classname = {idx_to_class[index] for index in top_indices}\n",
        "    \n",
        "    return top_probs, top_classname    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_685vsvbXaew"
      },
      "source": [
        "# Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gqNirA-VXaew"
      },
      "outputs": [],
      "source": [
        "training_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "testing_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the datasets with torchvision.datasets.ImageFolder object\n",
        "train_dataset = datasets.Flowers102(root = './dataset', split = 'train', transform = training_transforms, download = True)\n",
        "valid_dataset = datasets.Flowers102(root = './dataset', split = 'val', transform = validation_transforms, download = True)\n",
        "test_dataset = datasets.Flowers102(root = './dataset', split = 'test', transform = testing_transforms, download = True)\n",
        "\n",
        "# Instantiate loader objects to facilitate processing\n",
        "\n",
        "\n",
        "# Define the torch.utils.data.DataLoader() object with the ImageFolder object\n",
        "# Dataloader is a generator to read from ImageFolder and generate them into batch-by-batch\n",
        "# Only shuffle during trianing, validation and testing no shuffles\n",
        "# the batchsize for training and tesitng no need to be the same\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = tr_batchsize,\n",
        "                                           shuffle = True)\n",
        "\n",
        "validate_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
        "                                           batch_size = val_test_batchsize)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = val_test_batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hUdVmQ1-ajZb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo6JjGOiXaex"
      },
      "source": [
        "# Pretrained VGG\n",
        "\n",
        "In future this will be replaced with our own model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWmJtZd2Xaex",
        "outputId": "c58cb0d0-809c-4837-bd24-3e7d01c0d14c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG16(\n",
              "  (layer1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer5): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer6): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer7): Sequential(\n",
              "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer8): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer9): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer10): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer11): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer12): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (layer13): Sequential(\n",
              "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (2): ReLU()\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=102, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "model = VGG16()\n",
        "model.to(deviceFlag)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DclH6p0JXaey"
      },
      "outputs": [],
      "source": [
        "for params in model.parameters():\n",
        "    params.requries_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuQ_KXFXae0"
      },
      "source": [
        "# Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "kgDNGJAxXae0"
      },
      "outputs": [],
      "source": [
        "# Negative Log Likelihood Loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Cross Entropy Loss\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer 1\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "# optimizer 2\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfHUyA2BXae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456d809d-f34e-47e3-fb71-95af18cdb231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training batchsize is 64.\n"
          ]
        }
      ],
      "source": [
        "train_eval(model, train_loader, validate_loader, criterion, optimizer, epochs, deviceFlag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMhsTKf_Xae2"
      },
      "outputs": [],
      "source": [
        "test_acc(model, test_loader, deviceFlag)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deviceFlag"
      ],
      "metadata": {
        "id": "DLS4GGfxtAex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total_step = len(train_loader)\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     for i, (images, labels) in enumerate(train_loader):  \n",
        "#         # Move tensors to the configured device\n",
        "#         images = images.to(deviceFlag)\n",
        "#         labels = labels.to(deviceFlag)\n",
        "        \n",
        "#         # Forward pass\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "        \n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "#                    .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "#     # Validation\n",
        "#     with torch.no_grad():\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "#         for images, labels in validate_loader:\n",
        "#             images = images.to(deviceFlag)\n",
        "#             labels = labels.to(deviceFlag)\n",
        "#             outputs = model(images)\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#             del images, labels, outputs\n",
        "    \n",
        "#         print('Accuracy of the network on the {} validation images: {} %'.format(total, 100 * correct / total)) \n"
      ],
      "metadata": {
        "id": "-Ewf6DqpsY19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for images, labels in test_loader:\n",
        "#         images = images.to(deviceFlag)\n",
        "#         labels = labels.to(deviceFlag)\n",
        "#         outputs = model(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         del images, labels, outputs\n",
        "\n",
        "#     print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))   "
      ],
      "metadata": {
        "id": "nOkitkfYsZHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}