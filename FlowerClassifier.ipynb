{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W9QpA0RFXaeG",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.123535900Z",
     "start_time": "2023-05-07T04:16:27.569950700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hxEFQSjDXaeS",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.139783400Z",
     "start_time": "2023-05-07T04:16:30.124537Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_batchsize = 32\n",
    "val_test_batchsize = 16\n",
    "epochs = 60\n",
    "lr = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utaO7saJXaeU",
    "outputId": "32e2fabc-b3a8-4c1f-be8a-48a55ee91cb5",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.172390400Z",
     "start_time": "2023-05-07T04:16:30.155347500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs.\n",
      "Now the deivce is set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# By defalt, set device to the CPU\n",
    "deviceFlag = torch.device('cpu')\n",
    "\n",
    "# Default is CPU, but as long as GPU is avaliable, then use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Found {torch.cuda.device_count()} GPUs.')\n",
    "    deviceFlag = torch.device('cuda:0') # Manually pick your cuda device. By default is 'cuda:0'\n",
    "\n",
    "print(f'Now the deivce is set to {deviceFlag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading and Transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gqNirA-VXaew",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.376858200Z",
     "start_time": "2023-05-07T04:16:30.176470500Z"
    }
   },
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    # Randomly rotate it 90 degrees\n",
    "    transforms.RandomRotation(90),\n",
    "    # Randomly crop an area of the flower of size 224x224\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    # Flip it horizontally, or don't\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Flip it vertically, or don't\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # Convert the image to a Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the Tensor values so that they're easier for the\n",
    "    # model to train from\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], # RGB mean & std estied on ImageNet\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the datasets of the Flower102 images\n",
    "train_dataset = datasets.Flowers102(root = './dataset', split = 'train', transform = training_transforms, download = True)\n",
    "valid_dataset = datasets.Flowers102(root = './dataset', split = 'val', transform = validation_transforms, download = True)\n",
    "test_dataset = datasets.Flowers102(root = './dataset', split = 'test', transform = testing_transforms, download = True)\n",
    "\n",
    "\n",
    "# Create the loaders for the datasets, to be used to train, validate and test the model\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = tr_batchsize,\n",
    "                                           shuffle = True)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                           batch_size = val_test_batchsize)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = val_test_batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hUdVmQ1-ajZb",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.392232500Z",
     "start_time": "2023-05-07T04:16:30.377860600Z"
    }
   },
   "outputs": [],
   "source": [
    "import model_flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWmJtZd2Xaex",
    "outputId": "675ce95a-9a6c-414e-da1c-aa6d7a4bcf50",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:30.960667Z",
     "start_time": "2023-05-07T04:16:30.393250500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "FlowerModel(\n  (layer1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU()\n    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU()\n    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU()\n    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): ReLU()\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=100352, out_features=1024, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=1024, out_features=512, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=512, out_features=102, bias=True)\n    (6): LogSoftmax(dim=1)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_flower.FlowerModel()\n",
    "model.to(deviceFlag)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DclH6p0JXaey",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:31.004541300Z",
     "start_time": "2023-05-07T04:16:30.962671Z"
    }
   },
   "outputs": [],
   "source": [
    "# for params in model.parameters():\n",
    "#     params.requries_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNuQ_KXFXae0"
   },
   "source": [
    "# Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kgDNGJAxXae0",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:31.012681Z",
     "start_time": "2023-05-07T04:16:30.979014600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Negative Log Likelihood Loss\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer 1\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "# optimizer 2\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = 0.005, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import model_train\n",
    "import model_test"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7C7lTL2ajQJm",
    "outputId": "b568fa12-d25f-447e-934b-0a4ef507b577",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:16:31.013686400Z",
     "start_time": "2023-05-07T04:16:30.992825400Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] Epoch 1 completed on Batch 32 in 13.2895 seconds (13.2895 in total) \n",
      "\n",
      "[2/60] Epoch 2 completed on Batch 64 in 10.4957 seconds (23.7852 in total) \n",
      "\n",
      "[3/60] Epoch 3 completed on Batch 96 in 10.5799 seconds (34.3651 in total) \n",
      "\n",
      "[4/60] Batch: 100... Training Loss since last stepped validation: 4.2370... Validation Loss: 3.8870... Validation Accuracy: 0.0918\n",
      "[4/60] Epoch 4 completed on Batch 128 in 19.3139 seconds (53.6790 in total) \n",
      "\n",
      "[5/60] Epoch 5 completed on Batch 160 in 10.5576 seconds (64.2366 in total) \n",
      "\n",
      "[6/60] Epoch 6 completed on Batch 192 in 10.5125 seconds (74.7501 in total) \n",
      "\n",
      "[7/60] Batch: 200... Training Loss since last stepped validation: 3.5690... Validation Loss: 3.4731... Validation Accuracy: 0.1615\n",
      "[7/60] Epoch 7 completed on Batch 224 in 19.2789 seconds (94.0290 in total) \n",
      "\n",
      "[8/60] Epoch 8 completed on Batch 256 in 10.6297 seconds (104.6588 in total) \n",
      "\n",
      "[9/60] Epoch 9 completed on Batch 288 in 10.5968 seconds (115.2556 in total) \n",
      "\n",
      "[10/60] Batch: 300... Training Loss since last stepped validation: 3.1590... Validation Loss: 3.2170... Validation Accuracy: 0.2334\n",
      "[10/60] Epoch 10 completed on Batch 320 in 19.1782 seconds (134.4338 in total) \n",
      "\n",
      "[11/60] Epoch 11 completed on Batch 352 in 10.5865 seconds (145.0203 in total) \n",
      "\n",
      "[12/60] Epoch 12 completed on Batch 384 in 10.5573 seconds (155.5776 in total) \n",
      "\n",
      "[13/60] Batch: 400... Training Loss since last stepped validation: 2.8787... Validation Loss: 3.1264... Validation Accuracy: 0.2656\n",
      "[13/60] Epoch 13 completed on Batch 416 in 19.1354 seconds (174.7130 in total) \n",
      "\n",
      "[14/60] Epoch 14 completed on Batch 448 in 10.6676 seconds (185.3805 in total) \n",
      "\n",
      "[15/60] Epoch 15 completed on Batch 480 in 10.5924 seconds (195.9730 in total) \n",
      "\n",
      "[16/60] Batch: 500... Training Loss since last stepped validation: 2.6147... Validation Loss: 2.8961... Validation Accuracy: 0.3083\n",
      "[16/60] Epoch 16 completed on Batch 512 in 19.7389 seconds (215.7118 in total) \n",
      "\n",
      "[17/60] Epoch 17 completed on Batch 544 in 11.5004 seconds (227.2123 in total) \n",
      "\n",
      "[18/60] Epoch 18 completed on Batch 576 in 10.5693 seconds (237.7816 in total) \n",
      "\n",
      "[19/60] Batch: 600... Training Loss since last stepped validation: 2.4247... Validation Loss: 2.9453... Validation Accuracy: 0.3197\n",
      "[19/60] Epoch 19 completed on Batch 608 in 19.2530 seconds (257.0346 in total) \n",
      "\n",
      "[20/60] Epoch 20 completed on Batch 640 in 10.4784 seconds (267.5130 in total) \n",
      "\n",
      "[21/60] Epoch 21 completed on Batch 672 in 10.4790 seconds (277.9920 in total) \n",
      "\n",
      "[22/60] Batch: 700... Training Loss since last stepped validation: 2.2468... Validation Loss: 2.8116... Validation Accuracy: 0.3444\n",
      "[22/60] Epoch 22 completed on Batch 704 in 19.0216 seconds (297.0136 in total) \n",
      "\n",
      "[23/60] Epoch 23 completed on Batch 736 in 10.6766 seconds (307.6902 in total) \n",
      "\n",
      "[24/60] Epoch 24 completed on Batch 768 in 10.6732 seconds (318.3633 in total) \n",
      "\n",
      "[25/60] Batch: 800... Training Loss since last stepped validation: 2.1285... Validation Loss: 2.8357... Validation Accuracy: 0.3457\n",
      "[25/60] Epoch 25 completed on Batch 800 in 19.1407 seconds (337.5041 in total) \n",
      "\n",
      "[26/60] Epoch 26 completed on Batch 832 in 10.9137 seconds (348.4177 in total) \n",
      "\n",
      "[27/60] Epoch 27 completed on Batch 864 in 10.6181 seconds (359.0358 in total) \n",
      "\n",
      "[28/60] Epoch 28 completed on Batch 896 in 10.5916 seconds (369.6274 in total) \n",
      "\n",
      "[29/60] Batch: 900... Training Loss since last stepped validation: 1.9924... Validation Loss: 2.7581... Validation Accuracy: 0.3613\n",
      "[29/60] Epoch 29 completed on Batch 928 in 19.0939 seconds (388.7213 in total) \n",
      "\n",
      "[30/60] Epoch 30 completed on Batch 960 in 10.5034 seconds (399.2247 in total) \n",
      "\n",
      "[31/60] Epoch 31 completed on Batch 992 in 10.7003 seconds (409.9250 in total) \n",
      "\n",
      "[32/60] Batch: 1000... Training Loss since last stepped validation: 1.8843... Validation Loss: 2.8092... Validation Accuracy: 0.3558\n",
      "[32/60] Epoch 32 completed on Batch 1024 in 19.2066 seconds (429.1317 in total) \n",
      "\n",
      "[33/60] Epoch 33 completed on Batch 1056 in 10.9348 seconds (440.0664 in total) \n",
      "\n",
      "[34/60] Epoch 34 completed on Batch 1088 in 11.0351 seconds (451.1016 in total) \n",
      "\n",
      "[35/60] Batch: 1100... Training Loss since last stepped validation: 1.7885... Validation Loss: 2.9124... Validation Accuracy: 0.3548\n",
      "[35/60] Epoch 35 completed on Batch 1120 in 19.2346 seconds (470.3362 in total) \n",
      "\n",
      "[36/60] Epoch 36 completed on Batch 1152 in 10.5422 seconds (480.8784 in total) \n",
      "\n",
      "[37/60] Epoch 37 completed on Batch 1184 in 10.5573 seconds (491.4358 in total) \n",
      "\n",
      "[38/60] Batch: 1200... Training Loss since last stepped validation: 1.6508... Validation Loss: 2.8911... Validation Accuracy: 0.3659\n",
      "[38/60] Epoch 38 completed on Batch 1216 in 19.1657 seconds (510.6014 in total) \n",
      "\n",
      "[39/60] Epoch 39 completed on Batch 1248 in 10.5654 seconds (521.1668 in total) \n",
      "\n",
      "[40/60] Epoch 40 completed on Batch 1280 in 10.5383 seconds (531.7051 in total) \n",
      "\n",
      "[41/60] Batch: 1300... Training Loss since last stepped validation: 1.6408... Validation Loss: 2.8596... Validation Accuracy: 0.3822\n",
      "[41/60] Epoch 41 completed on Batch 1312 in 19.0124 seconds (550.7175 in total) \n",
      "\n",
      "[42/60] Epoch 42 completed on Batch 1344 in 10.5285 seconds (561.2460 in total) \n",
      "\n",
      "[43/60] Epoch 43 completed on Batch 1376 in 10.4803 seconds (571.7263 in total) \n",
      "\n",
      "[44/60] Batch: 1400... Training Loss since last stepped validation: 1.5623... Validation Loss: 2.7990... Validation Accuracy: 0.4085\n",
      "[44/60] Epoch 44 completed on Batch 1408 in 19.1459 seconds (590.8722 in total) \n",
      "\n",
      "[45/60] Epoch 45 completed on Batch 1440 in 10.5477 seconds (601.4199 in total) \n",
      "\n",
      "[46/60] Epoch 46 completed on Batch 1472 in 10.5979 seconds (612.0178 in total) \n",
      "\n",
      "[47/60] Batch: 1500... Training Loss since last stepped validation: 1.4433... Validation Loss: 2.8897... Validation Accuracy: 0.3887\n",
      "[47/60] Epoch 47 completed on Batch 1504 in 19.2295 seconds (631.2473 in total) \n",
      "\n",
      "[48/60] Epoch 48 completed on Batch 1536 in 10.5593 seconds (641.8066 in total) \n",
      "\n",
      "[49/60] Epoch 49 completed on Batch 1568 in 10.5152 seconds (652.3217 in total) \n",
      "\n",
      "[50/60] Batch: 1600... Training Loss since last stepped validation: 1.3826... Validation Loss: 2.8460... Validation Accuracy: 0.4046\n",
      "[50/60] Epoch 50 completed on Batch 1600 in 19.2662 seconds (671.5879 in total) \n",
      "\n",
      "[51/60] Epoch 51 completed on Batch 1632 in 10.7489 seconds (682.3368 in total) \n",
      "\n",
      "[52/60] Epoch 52 completed on Batch 1664 in 11.1424 seconds (693.4791 in total) \n",
      "\n",
      "[53/60] Epoch 53 completed on Batch 1696 in 10.9848 seconds (704.4640 in total) \n",
      "\n",
      "[54/60] Batch: 1700... Training Loss since last stepped validation: 1.3409... Validation Loss: 3.1286... Validation Accuracy: 0.3789\n",
      "[54/60] Epoch 54 completed on Batch 1728 in 19.6340 seconds (724.0989 in total) \n",
      "\n",
      "[55/60] Epoch 55 completed on Batch 1760 in 10.7625 seconds (734.8614 in total) \n",
      "\n",
      "[56/60] Epoch 56 completed on Batch 1792 in 10.7245 seconds (745.5860 in total) \n",
      "\n",
      "[57/60] Batch: 1800... Training Loss since last stepped validation: 1.2629... Validation Loss: 3.0055... Validation Accuracy: 0.3776\n",
      "[57/60] Epoch 57 completed on Batch 1824 in 19.2759 seconds (764.8618 in total) \n",
      "\n",
      "[58/60] Epoch 58 completed on Batch 1856 in 10.7136 seconds (775.5754 in total) \n",
      "\n",
      "[59/60] Epoch 59 completed on Batch 1888 in 10.7021 seconds (786.2774 in total) \n",
      "\n",
      "[60/60] Batch: 1900... Training Loss since last stepped validation: 1.2053... Validation Loss: 3.0096... Validation Accuracy: 0.3936\n",
      "[60/60] Epoch 60 completed on Batch 1920 in 19.4107 seconds (805.6882 in total) \n",
      "\n",
      "\n",
      "Training Complete in 805.6882 seconds.\n",
      "\n",
      "Validation of all 1920 Batches:\n",
      "Training Loss: 2.1172... Validation Loss: 3.0193... Validation Accuracy: 0.4027\n"
     ]
    },
    {
     "data": {
      "text/plain": "1920"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.train_classifier(model, train_loader, validate_loader, optimizer, criterion,\n",
    "                             device_flag=deviceFlag, epochs=epochs,\n",
    "                             validate_steps=100, validate_stepped=True, validate_epoch=False,\n",
    "                             validate_end=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T04:30:05.249329500Z",
     "start_time": "2023-05-07T04:16:31.010661Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3604545295238495 2216 / 6149 correctly predicted.\n"
     ]
    }
   ],
   "source": [
    "model_test.test_accuracy(model, test_loader, device_flag=deviceFlag)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T04:35:57.774546900Z",
     "start_time": "2023-05-07T04:35:03.432592600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T04:31:02.103336500Z",
     "start_time": "2023-05-07T04:31:02.086997300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DLS4GGfxtAex",
    "ExecuteTime": {
     "end_time": "2023-05-07T04:31:04.681353600Z",
     "start_time": "2023-05-07T04:31:02.103336500Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/\" + str(datetime.datetime.now()).replace(\":\",\"-\")\n",
    "           + f\" b{tr_batchsize}-e{epochs}-lr{lr}\" \"-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Stop Run All here\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Stop Run All here\n",
    "assert False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T04:31:05.756912300Z",
     "start_time": "2023-05-07T04:31:04.683385200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'model_test' from 'Z:\\\\Files\\\\University\\\\Exams\\\\INT2\\\\GitHub Project\\\\model_test.py'>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload imports in the case that they are changed\n",
    "from importlib import reload\n",
    "\n",
    "# If not loaded into cache yet, import them\n",
    "import model_flower\n",
    "import model_train\n",
    "import model_test\n",
    "\n",
    "reload(model_flower)\n",
    "reload(model_train)\n",
    "reload(model_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T04:34:59.990920800Z",
     "start_time": "2023-05-07T04:34:59.968950600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ewf6DqpsY19"
   },
   "outputs": [],
   "source": [
    "# total_step = len(train_loader)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i, (images, labels) in enumerate(train_loader):  \n",
    "#         # Move tensors to the configured device\n",
    "#         images = images.to(deviceFlag)\n",
    "#         labels = labels.to(deviceFlag)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "#     # Validation\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for images, labels in validate_loader:\n",
    "#             images = images.to(deviceFlag)\n",
    "#             labels = labels.to(deviceFlag)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             del images, labels, outputs\n",
    "    \n",
    "#         print('Accuracy of the network on the {} validation images: {} %'.format(total, 100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOkitkfYsZHM"
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         images = images.to(deviceFlag)\n",
    "#         labels = labels.to(deviceFlag)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#         del images, labels, outputs\n",
    "\n",
    "#     print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))   "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
